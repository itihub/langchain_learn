{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# 手动设置环境变量\n",
    "# os.environ[\"OPENAI_API_BASE_URL\"] = \"https://ai-yyds.com/v1\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXXXXXXXXXXXXXXXX0bDb\"\n",
    "# os.environ[\"MODEL_NAME\"] = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 记忆（Memory）工具\n",
    "LangChain 提供了多种记忆（Memory）工具，用于在对话或链的执行过程中保持状态。这些工具可以帮助语言模型（LLM）记住之前的交互，从而生成更连贯和上下文相关的响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同的Memory工具\n",
    "利用内存实现短时记忆\n",
    "利用Entity memory构建实体记忆\n",
    "利用知识图谱来构建记忆\n",
    "利用对话摘要来兼容内存中的长对话\n",
    "使用token来刷新内存缓冲区\n",
    "使用向量数据库实现长时记忆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于内存实现短时记忆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationBufferMemory\n",
    "ConversationBufferMemory 是最简单的记忆形式，它将所有对话消息存储在一个缓冲区中。\n",
    "它保留了完整的对话历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 创建一个 ConversationBufferMemory 实例\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 添加对话消息\n",
    "memory.chat_memory.add_user_message(\"你好，我是人类！\")\n",
    "memory.chat_memory.add_ai_message(\"你好，我是AI，有什么可以帮助你的吗？\")\n",
    "\n",
    "# 从内存中加载记忆变量\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationBufferWindowMemory \n",
    "ConversationBufferWindowMemory只保留最近的 k 个交互。\n",
    "它适用于需要保留有限上下文的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现一个最近的对话窗口，超过窗口条数的对话将被删除\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 创建一个 ConversationBufferWindowMemory 实例，并指定 k 的值为 2。\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "# 将当前对话的上下文保存到内存中\n",
    "memory.save_context({\"input\":\"你好，我是人类!\"}, {\"output\":\"你好，我是AI，有什么可以帮助你的吗？\"})\n",
    "memory.save_context({\"input\":\"我想吃鸡肉\"}, {\"output\":\"好的，我帮你找找鸡肉的做法\"})\n",
    "\n",
    "# 从内存中加载记忆变量\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建记忆实体概念清单\n",
    "ConversationEntityMemory 是一种专门用于跟踪对话中实体的记忆工具。它不仅能记住对话的历史，还能提取和总结对话中提到的实体，并随着对话的进行不断更新这些实体的信息。\n",
    "+ 功能：\n",
    "  + 实体跟踪：\n",
    "    + ConversationEntityMemory 的主要功能是识别和跟踪对话中出现的实体。\n",
    "    + 它使用语言模型（LLM）来提取实体信息，并随着对话的进行更新这些信息。\n",
    "  + 上下文提供：\n",
    "    + 通过维护实体信息，ConversationEntityMemory 可以为 LLM 提供更丰富的上下文，从而生成更连贯和相关的响应。\n",
    "+ 工作原理：\n",
    "  + 实体提取：\n",
    "    + 在对话过程中，ConversationEntityMemory 使用 LLM 从对话文本中提取实体。\n",
    "  + 实体存储：\n",
    "    + 提取出的实体及其相关信息被存储在内存中。\n",
    "  + 实体更新：\n",
    "    + 随着对话的进行，ConversationEntityMemory 会不断更新已存储的实体信息，以反映对话的最新状态。\n",
    "  + 上下文提供：\n",
    "    + 当 LLM 需要生成响应时，ConversationEntityMemory 会将相关的实体信息作为上下文提供给 LLM。\n",
    "+ 使用场景：\n",
    "  + 需要跟踪特定实体的对话：\n",
    "    + 例如，在客户服务对话中，跟踪客户的订单信息、产品信息等。\n",
    "  + 需要维护实体状态的对话：\n",
    "    + 例如，在虚拟助手对话中，跟踪用户的偏好、日程安排等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 初始化实体内存\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "_input = {\n",
    "    \"input\":\"胡八一和王胖子雪莉杨经常在一起冒险，合称盗墓铁三角.\"\n",
    "}\n",
    "\n",
    "# 从对话的内容中提取实体概念\n",
    "# memory.load_memory_variables(_input)\n",
    "\n",
    "# 写入记忆中\n",
    "memory.save_context(\n",
    "    _input,\n",
    "    {\n",
    "        \"output\":\"听起来很刺激，我也想加入他们！\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({\"input\":\"铁三角是谁?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用知识图谱构建记忆\n",
    "ConversationKGMemory 使用知识图谱（Knowledge Graph）来存储对话信息。\n",
    "它可以更好地理解实体之间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 创建一个 ConversationKGMemory 实例，并指定用于知识图谱操作的 LLM\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "\n",
    "# 添加一组对话记忆\n",
    "memory.save_context(\n",
    "    {\"input\":\"帮我找一下tomie\"},\n",
    "    {\"output\":\"对不起请问什么是tomie？\"}\n",
    ")\n",
    "\n",
    "# 添加一组对话记忆\n",
    "memory.save_context(\n",
    "    {\"input\":\"tomie是一个培训讲师\"},\n",
    "    {\"output\":\"好的，我知道了。\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({\"input\":\"tomie是谁?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取实体概念\n",
    "memory.get_current_entities(\"tomie最喜欢做什么事?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取三元知识组\n",
    "memory.get_knowledge_triplets(\"tomie最喜欢打游戏\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 长对话在内存中的处理方式：总结摘要以及token计算\n",
    "在 LangChain 中，ConversationSummaryMemory 是一种专门用于处理长对话的记忆工具。它通过总结对话的早期部分，只保留最近的交互，从而有效地管理对话的上下文，避免因对话过长而超出语言模型（LLM）的上下文窗口限制。\n",
    "在 LangChain 中，ConversationSummaryMemory 是一种专门用于处理长对话的记忆工具。它通过总结对话的早期部分，只保留最近的交互，从而有效地管理对话的上下文，避免因对话过长而超出语言模型（LLM）的上下文窗口限制。\n",
    "\n",
    "以下是关于 ConversationSummaryMemory 的详细解释：\n",
    "+ 功能：\n",
    "  + 对话摘要：\n",
    "    + ConversationSummaryMemory 的主要功能是总结对话的早期部分，将长对话压缩成更短的摘要。\n",
    "    + 它使用 LLM 来生成对话的摘要。\n",
    "  + 上下文管理：\n",
    "    + 通过维护对话摘要，ConversationSummaryMemory 可以为 LLM 提供更精简的上下文，从而生成更连贯和相关的响应。\n",
    "    + 它有效地解决了长对话中上下文窗口限制的问题。\n",
    "+ 工作原理：\n",
    "  + 对话存储：\n",
    "    + ConversationSummaryMemory 像 ConversationBufferMemory 一样，存储对话的完整历史记录。\n",
    "  + 摘要生成：\n",
    "    + 当对话长度超过一定阈值时，ConversationSummaryMemory 使用 LLM 生成对话的摘要。\n",
    "  + 上下文提供：\n",
    "    + 在后续的对话中，ConversationSummaryMemory 将摘要和最近的交互作为上下文提供给 LLM。\n",
    "+ 使用场景：\n",
    "  + 长对话：\n",
    "    + 当对话可能很长，并且需要保留对话的整体上下文时。\n",
    "  + 资源有限：\n",
    "    + 当计算资源有限，无法存储完整的对话历史记录时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "memory.save_context(\n",
    "    {\"input\":\"帮我找一下tomie\"},\n",
    "    {\"output\":\"对不起请问什么是tomie？\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"tomie是一个培训讲师\"},\n",
    "    {\"output\":\"好的，我知道了。\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看所有的记忆内容\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取完整的对话内容\n",
    "messages = memory.chat_memory.messages\n",
    "# print(messages)\n",
    "\n",
    "# 产生对话摘要\n",
    "memory.predict_new_summary(messages, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用ChatMessageHistory来快速获得对话摘要\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 定义历史对话记录\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"你好，我是人类！\")\n",
    "history.add_ai_message(\"你好，我是AI小丸子，有什么可以帮助你的吗？\")\n",
    "\n",
    "memory = ConversationSummaryMemory.from_messages(\n",
    "   llm=llm,\n",
    "   chat_memory=history,\n",
    "   return_messages=True\n",
    ")\n",
    "\n",
    "# 方式二：在初始化中把对话放入buffer中\n",
    "# memory = ConversationSummaryMemory(\n",
    "#     llm=llm,\n",
    "#     return_messages=True,\n",
    "#     buffer=\"\\nThe AI introduces itself as AI Little Maruko and asks if there is anything it can help the human with.\",\n",
    "#     chat_memory=hisiory\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看内存中的内容 memory.buffer\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationSummaryBufferMemory\n",
    "当对话持续进行且对话内容很多的时候，可以使用ConversationSummaryBufferMemory来存储对话摘要\n",
    "ConversationSummaryBufferMemory是一种结合了ConversationBufferMemory和ConversationSummaryMemory特点的记忆工具，它的目的是在处理长对话时，既保持对话的连贯性，又避免因对话过长而超出语言模型（LLM）的上下文窗口限制。\n",
    "这是一种非常有用的方式，它会根据token的数量来自动判断是否需要进行摘要\n",
    "当token数量超过阈值的时候，会自动进行摘要\n",
    "在缓冲区中，会保留最近的k条对话\n",
    "比较久的对话会被删除，在删除前会进行摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    # 设置触发摘要生成的最大令牌数限制。可以根据LLM的上下文窗口的大小进行调整。\n",
    "    max_token_limit=10,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 添加对话内容\n",
    "memory.save_context(\n",
    "    {\"input\":\"帮我找一下tomie\"},\n",
    "    {\"output\":\"对不起请问什么是tomie？\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"tomie是一个培训讲师\"},\n",
    "    {\"output\":\"好的，我知道了。\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"今天他要讲一门关于RAG的课程\"},\n",
    "    {\"output\":\"好的，我知道了。需要RAG的资料吗？\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationTokenBufferMemory使用token长度来决定什么时候刷新内存\n",
    "在 LangChain 中，ConversationTokenBufferMemory 是一种用于管理对话历史记录的记忆工具，它与 ConversationBufferMemory 类似，但它使用令牌（tokens）的数量而不是消息的数量来确定何时截断对话历史记录。\n",
    "\n",
    "以下是关于 ConversationTokenBufferMemory 的详细解释：\n",
    "\n",
    "+ 功能：\n",
    "  + 令牌缓冲区：\n",
    "    + ConversationTokenBufferMemory 将对话历史记录存储在一个缓冲区中，但它会跟踪对话中使用的令牌数量。\n",
    "    + 当令牌数量超过指定的限制时，它会从缓冲区中删除最早的消息，以保持在限制范围内。\n",
    "  + 上下文管理：\n",
    "    + 通过限制对话历史记录的令牌数量，ConversationTokenBufferMemory 可以有效地管理对话的上下文，避免因对话过长而超出语言模型（LLM）的上下文窗口限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    # 指定用于令牌计数的 LLM\n",
    "    llm=llm,\n",
    "    # 设置最大令牌数限制\n",
    "    max_token_limit=50\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\":\"帮我找一下tomie\"},\n",
    "    {\"output\":\"对不起请问什么是tomie？\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"tomie是一个培训讲师\"},\n",
    "    {\"output\":\"好的，我知道了。\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"今天他要讲一门关于RAG的课程\"},\n",
    "    {\"output\":\"好的，我知道了。需要RAG的资料吗？\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"不需要资料了，谢谢\"},\n",
    "    {\"output\":\"好的，那我就不打扰你了。\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 长时记忆的是实现方式\n",
    "\n",
    "通过向量数据库来存储之前的对话内容，有的向量数据库服务还提供自动摘要等，每次对话的时候，都会从向量数据库里查询最相关的文档或历史对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\":\"帮我找一下tomie\"},\n",
    "    {\"output\":\"对不起请问什么是tomie？\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"tomie是一个培训讲师\"},\n",
    "    {\"output\":\"好的，我知道了。\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"今天他要讲一门关于RAG的课程\"},\n",
    "    {\"output\":\"好的，我知道了。需要RAG的资料吗？\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"不需要资料了，谢谢\"},\n",
    "    {\"output\":\"好的，那我就不打扰你了。\"}\n",
    ")\n",
    "\n",
    "# 定义向量数据库\n",
    "vector_store = FAISS.from_texts(\n",
    "    memory.buffer.split(\"\\n\"),\n",
    "    OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# 将向量数据库存在本地文件\n",
    "FAISS.save_local(vector_store, \"test_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行相似性搜索\n",
    "FAISS.load_local(\"test_faiss\", \n",
    "                 OpenAIEmbeddings(model=\"text-embedding-3-small\"), \n",
    "                 allow_dangerous_deserialization=True).similarity_search(\"tomie是什么职业?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "# 加载长期记忆文件\n",
    "r1 = FAISS.load_local(\"test_faiss\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "r2 = r1.as_retriever(\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "memory2 = VectorStoreRetrieverMemory(\n",
    "    retriever=r2\n",
    ")\n",
    "memory2.load_memory_variables({\"prompt\":\"tomie是什么职业?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
