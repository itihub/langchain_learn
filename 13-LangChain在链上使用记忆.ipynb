{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# 手动设置环境变量\n",
    "# os.environ[\"OPENAI_API_BASE_URL\"] = \"https://ai-yyds.com/v1\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXXXXXXXXXXXXXXXX0bDb\"\n",
    "# os.environ[\"MODEL_NAME\"] = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在链上使用记忆\n",
    "在 LangChain 中，记忆（Memory）是指链（Chain）或代理（Agent）记住先前对话或交互的能力。这对于构建聊天机器人、对话系统和其他需要保持上下文的应用程序至关重要。LangChain 提供了多种记忆工具，允许您在链中使用它们，以实现不同的记忆行为。\n",
    "+ 为什么在链中使用记忆？\n",
    "  + 保持对话上下文：\n",
    "    + 记忆可以帮助链记住之前的对话内容，并在生成响应时考虑这些内容，从而使对话更加自然和连贯。\n",
    "  + 提高模型的准确性：\n",
    "    + 记忆可以帮助链更好地理解用户的意图，并生成更加准确的响应。\n",
    "  + 实现更复杂的对话逻辑：\n",
    "    + 记忆可以帮助链跟踪对话中的实体、关系和其他信息，从而实现更复杂的对话逻辑。\n",
    "\n",
    "+ LangChain 中的记忆类型\n",
    "LangChain 提供了多种记忆类型，每种类型都有不同的特点和适用场景：\n",
    "  + ConversationBufferMemory：\n",
    "    + 这是最简单的记忆形式，它将所有对话消息存储在一个缓冲区中。\n",
    "    + 它保留了完整的对话历史记录。\n",
    "    + 适用于短对话。\n",
    "  + ConversationBufferWindowMemory：\n",
    "    + 这种记忆形式只保留最近的“k”个交互。\n",
    "    + 它通过限制存储的对话历史记录来解决上下文窗口限制的问题。\n",
    "    + 适用于中等长度的对话。\n",
    "  + ConversationSummaryMemory：\n",
    "    + 这种记忆形式总结了对话的早期部分，只保留最近的交互。\n",
    "    + 它使用 LLM 来创建对话的摘要，从而减少了需要存储的令牌数量。\n",
    "    + 适用于长对话。\n",
    "  + ConversationKGMemory：\n",
    "    + 这种记忆形式使用知识图谱来存储对话信息。\n",
    "    + 它提取对话中的实体和关系，并将其存储在知识图谱中。\n",
    "    + 适用于需要进行知识推理的对话。\n",
    "  + ConversationEntityMemory：\n",
    "    + 跟踪对话中提到的实体，并随着对话的进行不断更新这些实体的信息。\n",
    "    + 使用LLM从对话文本中提取实体。\n",
    "    + 适用于需要跟踪特定实体的对话。\n",
    "  + ConversationSummaryBufferMemory：\n",
    "    + 在内存中保留最近交互的缓冲区，同时将较早的交互编译成摘要。\n",
    "    + 使用token长度而不是交互次数来确定何时刷新交互。\n",
    "    + 适用于长对话，并且需要尽量保持对话的完整上下文时。\n",
    "  + ConversationTokenBufferMemory：\n",
    "    + 它与 ConversationBufferMemory 类似，但它使用令牌（tokens）的数量而不是消息的数量来确定何时截断对话历史记录。\n",
    "    + 适用于需要精确控制上下文长度的对话。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain 使用记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 自定义模板\n",
    "template = \"\"\"你是一个可以和人类对话的机器人.\n",
    "{chat_history}\n",
    "人类:{human_input}\n",
    "机器人:\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(human_input=\"我最新喜欢我的世界这个游戏，你还记得我叫什么吗？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import  (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import  SystemMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"你好，我是一个可以和人类对话的机器人\",\n",
    "            role=\"system\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\",\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print(prompt.format(human_input=\"你好\",chat_history=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(human_input=\"我叫tomie，我是一个AI应用程序猿\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(input=\"帮我做个一日游攻略\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义一下，对其进行覆盖\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "template = \"\"\"下面是一段AI与人类的对话，AI会针对人类问题，提供尽可能详细的回答，如果AI不知道答案，会直接回复'人类老爷，我真的不知道'.\n",
    "当前对话:\n",
    "{history}\n",
    "Human:{input}\n",
    "AI助手:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"history\", \"input\"],\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(\n",
    "        ai_prefix=\"AI助手\",\n",
    "        return_messages=True,\n",
    "    ),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(input=\"你知道我叫什么名字吗?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同一个链合并使用多个memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    CombinedMemory\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 使用ConversationSummaryMemory对对话进行总结\n",
    "summary = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    input_key=\"input\"\n",
    ")\n",
    "# 使用ConversationBufferMemory对对话进行缓存\n",
    "cov_memory = ConversationBufferMemory(\n",
    "    memory_key=\"history_now\",\n",
    "    input_key=\"input\",\n",
    ")\n",
    "\n",
    "memory = CombinedMemory(\n",
    "    memories=[summary, cov_memory],\n",
    ")\n",
    "\n",
    "TEMPLATE = \"\"\"下面是一段AI与人类的对话，AI会针对人类问题，提供尽可能详细的回答，如果AI不知道答案，会直接回复'人类老爷，我真的不知道'.\n",
    "之前的对话摘要:\n",
    "{history}\n",
    "当前对话:\n",
    "{history_now}\n",
    "Human:{input}\n",
    "AI：\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=TEMPLATE,\n",
    "    input_variables=[\"history\", \"history_now\", \"input\"],\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"那ETH呢？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多参数链增加记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import  OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "with open(\"./sources/letter.txt\") as f:\n",
    "    text = f.read()\n",
    "#切分文本\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 20,\n",
    "    chunk_overlap = 5\n",
    ")\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "#使用openai的embedding\n",
    "embeddings = OpenAIEmbeddings()\n",
    "#使用chroma向量存储\n",
    "docssearch = Chroma.from_texts(\n",
    "    texts,\n",
    "    embeddings,\n",
    ")\n",
    "query = \"公司有什么新策略?\"\n",
    "docs = docssearch.similarity_search(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建问答对话链\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "template = \"\"\"下面是一段AI与人类的对话，AI会针对人类问题，提供尽可能详细的回答，如果AI不知道答案，会直接回复'人类老爷，我真的不知道'，参考一下相关文档以及历史对话信息，AI会据此组织最终回答内容.\n",
    "{context}\n",
    "{chat_history}\n",
    "Human:{human_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"chat_history\", \"human_input\"],\n",
    ")\n",
    "\n",
    "#使用ConversationBufferMemory对对话进行缓存 \n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "#加载对话链\n",
    "chain = load_qa_chain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "chain({\"input_documents\":docs, \"human_input\":\"公司的营销策略是什么？\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
