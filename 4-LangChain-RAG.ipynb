{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检索增强生成技术RAG\n",
    "RAG（检索增强生成）技术主要针对大型语言模型（LLMs）的以下几个缺点进行了改进：\n",
    "- 知识更新滞后：\n",
    "  - 大型语言模型通常基于大量历史数据进行训练，这导致它们的知识库存在时间上的滞后。对于最新的事件、信息或知识，模型可能无法提供准确的回答。\n",
    "  - RAG通过引入外部知识库，允许模型在生成响应时动态检索最新信息，从而弥补了这一缺陷。\n",
    "- 幻觉问题（Hallucination）\n",
    "  - 大型语言模型有时会生成看似合理但实际上不正确或不存在的信息，即“幻觉”。这可能是因为模型在训练数据中没有足够的证据支持其生成的答案，或者因为模型过度依赖于模式匹配。\n",
    "  - RAG通过将检索到的真实信息作为生成的基础，降低了模型产生幻觉的风险，提高了生成内容的可靠性。\n",
    "- 领域知识不足：\n",
    "  - 虽然大型语言模型在通用知识方面表现出色，但在特定领域或专业领域，它们的知识可能有限。\n",
    "  - RAG允许模型访问特定领域的知识库，从而增强其在这些领域的专业性，使其能够提供更准确和深入的回答。\n",
    "- 缺乏信息来源：\n",
    "  - 大型语言模型生成的文本通常缺乏明确的信息来源，这使得用户难以验证信息的真实性和可靠性。\n",
    "  - RAG可以提供检索到的文档或信息的链接，使用户能够追溯信息的来源，提高生成内容的可信度。\n",
    "总结来说，RAG技术通过将信息检索与文本生成相结合，有效地解决了大型语言模型在知识更新、幻觉问题、领域知识和信息来源方面的不足，提高了生成文本的质量和可靠性。\n",
    "\n",
    "RAG（Retrieval-Augmented Generation，检索增强生成）是一种将信息检索与文本生成相结合的技术，用于提高大型语言模型（LLMs）生成文本的质量和准确性。简单来说，RAG允许LLMs在生成回答之前，先从外部知识库中检索相关信息，然后利用这些信息来增强其生成的内容。\n",
    "\n",
    "以下是RAG的核心概念和工作原理：\n",
    "核心概念：\n",
    "- 检索（Retrieval）：\n",
    "  RAG系统首先根据用户的查询，从一个或多个外部知识库中检索相关文档或信息片段。\n",
    "  这些知识库可以是向量数据库、传统数据库、网页、文档集合等。\n",
    "- 增强（Augmentation）：\n",
    "  检索到的信息被整合到LLM的输入提示（prompt）中，作为额外的上下文。\n",
    "  这使得LLM能够访问其训练数据之外的最新或特定领域的信息。\n",
    "- 生成（Generation）：\n",
    "  LLM利用增强后的提示，生成包含检索到的相关信息的回答或文本。\n",
    "\n",
    "工作原理：\n",
    "- 用户查询：\n",
    "  用户向RAG系统提出一个问题或请求。\n",
    "- 信息检索：\n",
    "  系统使用检索模型（例如，基于向量相似度的检索）在知识库中查找与查询相关的文档。\n",
    "- 上下文增强：\n",
    "  检索到的文档被添加到LLM的输入提示中，为LLM提供额外的上下文信息。\n",
    "- 文本生成：\n",
    "  LLM根据增强后的提示，生成包含检索到的信息的回答或文本。\n",
    "\n",
    "RAG的优势：\n",
    "- 提高准确性：\n",
    "  通过访问外部知识，RAG可以减少LLM生成不准确或过时信息的风险。\n",
    "- 增强知识：\n",
    "  RAG允许LLM访问其训练数据中没有的特定领域或最新信息。\n",
    "- 提高可信度：\n",
    "  RAG可以提供信息来源，增加生成文本的可信度。\n",
    "- 减少幻觉：\n",
    "  通过检索实际的知识，来减少大型语言模型，自己“创造”信息的可能性。\n",
    "\n",
    "RAG的应用场景：\n",
    "- 问答系统：提供基于特定知识库的准确回答。\n",
    "- 内容创作：生成包含最新信息的文章、报告等。\n",
    "- 客户服务：提供基于产品文档或知识库的客户支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain中的RAG的实现\n",
    "在 LangChain 中实现 RAG关键组件\n",
    "- 文档加载器（Document Loaders）：LangChain 提供了多种文档加载器，用于从各种数据源加载文档\n",
    "- 文本分割器（Text Splitters）：加载的文档通常很长，需要分割成更小的块，以便嵌入模型和语言模型处理\n",
    "- 嵌入模型（Embeddings）：嵌入模型将文本转换为向量，以便计算文本之间的相似度。\n",
    "- 向量数据库（Vector Stores）：向量数据库用于存储和检索嵌入向量。\n",
    "- 检索器（Retrievers）：检索器将查询转换为嵌入向量，并在向量数据库中查找最相似的文本块。\n",
    "- 语言模型（Language Models）：语言模型将检索到的文本块和用户查询作为输入，生成回答。\n",
    "- 链（Chains）：LangChain 的链允许你将多个组件组合在一起，形成一个工作流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 文档加载器（loader）：让大模型具备实时学习能力\n",
    "- CSV loader：加载csv格式的文件\n",
    "- File directory：根据目录加载\n",
    "- Html loader：加载html格式的文件\n",
    "- Json loader：加载json格式的文件\n",
    "- Markdown loader：加载Markdown格式的文件\n",
    "- Pdf loader：加载pdf格式的文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 加载Markdown文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用loader来加载Markdown文本\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(file_path=\"./sources/loader.md\", encoding=\"utf-8\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 加载SCV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用loader来加载CSV文本\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# loader = CSVLoader(file_path='./sources/loader.csv', encoding=\"utf-8\")\n",
    "# 加载指定列\n",
    "loader = CSVLoader(file_path='./sources/loader.csv', source_column=\"Location\", encoding=\"utf-8\")\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 加载Excel文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖包，才能实现对excel文件的解析\n",
    "%pip install \"unstructured[xlsx]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用loader来加载Excel文本\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# 某个目录下，有excel文件，我们需要把这个目录下所有的excel文件都加载进来\n",
    "# 使用DirectoryLoader不会加载该目录下的.html和.rst文件\n",
    "loader = DirectoryLoader(\"./sources\", glob=\"*.xlsx\")\n",
    "xlsx = loader.load()\n",
    "print(xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 加载HTML文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖包\n",
    "%pip install unstructured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用loader来加载html文件\n",
    "# UnstructuredHTMLLoader 使用 unstructured 库来加载 HTML 文件。它能够处理更复杂的 HTML 结构，并提取出更干净的文本内容。\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# 把html源代码加载进来\n",
    "loader = UnstructuredHTMLLoader(\"./sources/loader.html\")\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BSHTMLLoader 使用 BeautifulSoup 库来解析 HTML 文件。它更适用于简单的 HTML 结构，并且可以方便地提取特定元素的内容。\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "# 只把html中文本加载进来\n",
    "bs_loader = BSHTMLLoader(file_path=\"./sources/loader.html\", open_encoding=\"utf-8\")\n",
    "data = bs_loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 加载Json文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用loader来加载json，需要先安装依赖\n",
    "%pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# 只把html中文本加载进来，jq_schema参数允许你使用 jq 查询语言来提取 JSON 文件中的特定内容。\n",
    "loader = JSONLoader(file_path=\"simple_prompt.json\", jq_schema=\".template\", text_content=False)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 加载PDF文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析pdf文件的依赖包\n",
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader加载pdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./sources/loader.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain文档转换\n",
    "- 文档切割器和按字符分割\n",
    "- 代码文档分割器\n",
    "- 按token分割文档\n",
    "- 文档总结、精炼、翻译\n",
    "  \n",
    "文档转换器原理\n",
    "1. 将文档分成小的，有意义的块（句子）\n",
    "2. 将小的块组合成一个更大的块，直到到达一定的大小\n",
    "3. 一旦到达一定的大小，接着开始创建与下一个块重叠的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 递归地按字符分割文本，并尝试保持语义完整性。\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 加载要切分的文档\n",
    "with open(file=\"./sources/test.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50, # 切分的文本块大小，一般通过长度函数计算\n",
    "    chunk_overlap=20, # 表示相邻文本块之间的重叠部分长度，一般通过长度函数计算\n",
    "    length_function=len, # 计算文本块的长度函数，也可以传递tokenize函数\n",
    "    add_start_index=True, # 是否在元数据中包含每个块的起始索引\n",
    ")\n",
    "\n",
    "documents = text_splitter.create_documents([text])\n",
    "for text in documents:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按字符分割文本\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 加载要切分的文档\n",
    "with open(file=\"./sources/test.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"。\", # 切割的标识字符，默认是\"\\n\\n\"\n",
    "    chunk_size=50, # 切分的每个文本块的最大长度（以字符数为单位）\n",
    "    chunk_overlap=20, # 切分的文本相邻块之间的重叠部分长度\n",
    "    length_function=len, # 长度函数，也可以传递tokenize函数\n",
    "    add_start_index=True, # 是否添加开始索引\n",
    "    is_separator_regex=False, # 是否使用正则表达式\n",
    ")\n",
    "\n",
    "documents = text_splitter.create_documents([text])\n",
    "for text in documents:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码文档切割\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# 支持解析的编程语言\n",
    "# [print(e.value) for e in Language]\n",
    "\n",
    "# 要切割的代码文档示例\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"hello world\")\n",
    "\n",
    "# 调用函数\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "py_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    ")\n",
    "\n",
    "python_docs = py_splitter.create_documents([PYTHON_CODE])\n",
    "print(python_docs)\n",
    "for docs in python_docs:\n",
    "    print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按token来分割文档\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 加载要切分的文档\n",
    "with open(file=\"./sources/test.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, # 切分的每个文本块的最大长度（以字符数为单位）\n",
    "    chunk_overlap=30, # 切分的文本相邻块之间的重叠部分长度\n",
    ")\n",
    "\n",
    "documents = text_splitter.create_documents([text])\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文档的总结、精炼、翻译\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "%pip install doctran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文档\n",
    "with open(file=\"./sources/test.txt\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "API_BASE = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "OPENAI_TOKEN_LIMIT = 8000\n",
    "\n",
    "from doctran import Doctran\n",
    "\n",
    "# Doctran是对openai的一层封装\n",
    "doctrans = Doctran(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_model=MODEL_NAME,\n",
    "    openai_token_limit=OPENAI_TOKEN_LIMIT,\n",
    ")\n",
    "\n",
    "documents = doctrans.parse(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结文档, token_limit参数是总结后的字数限制\n",
    "summary = documents.summarize(token_limit=100).execute()\n",
    "print(summary.transformed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 翻译问答\n",
    "translation = documents.translate(language=\"chinese\").execute()\n",
    "print(translation.transformed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精炼文档，删除除了某主题或关键词之外的内容，仅保留与主体相关的内容\n",
    "# 指定主题来进行精炼\n",
    "refined = documents.refine(topics=[\"marketing\",\"Development\"]).execute()\n",
    "print(refined.transformed_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-chain-wYHJBg1W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
