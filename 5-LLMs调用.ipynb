{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载环境变量\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用LangChain调用文本类模型,以OpenAI为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用openai的官方sdk\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    # api_base=os.getenv(\"OPENAI_API_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"介绍下你自己\"}\n",
    "]\n",
    "\n",
    "res = client.chat.completions.create(\n",
    "    model=os.getenv(\"MODEL_NAME\"),\n",
    "    messages=message,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用LangChain调用对话类模型,以ChatOpenAI为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用对话类，以openai为例\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "import os\n",
    "\n",
    "api_base = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    base_url=api_base,\n",
    "    api_key=api_key,\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    AIMessage(role=\"system\",content=\"你好，我是tomie！\"),\n",
    "    HumanMessage(role=\"user\",content=\"你好tomie，我是狗剩!\"),\n",
    "    AIMessage(role=\"system\",content=\"认识你很高兴!\"),\n",
    "    HumanMessage(role=\"user\",content=\"你知道我叫什么吗？\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### langchain内置的LLM支持情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LLMs](images/LLMs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain调用大模型流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM文本类大模型的流式输出方法\n",
    "\n",
    "from langchain_community.llms import OpenAI\n",
    "import os\n",
    "\n",
    "api_base = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "# 构造一个llm实例\n",
    "llm = OpenAI(\n",
    "    # openai_proxy=api_base,\n",
    "    openai_api_key=api_key,\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    \n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "# 流式输出\n",
    "for chunk in llm.stream(\"写一首关于秋天的诗歌\"):\n",
    "    # flush:缓冲区刷新禁用\n",
    "    print(chunk, end=\"\", flush=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Models的流式调用方法\n",
    "# 使用clade模型\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "api_base = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "# 构建一个ChatOpenAI实例\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=api_base,\n",
    "    openai_api_key=api_key,\n",
    "    model = \"claude-3-opus-20240229\",\n",
    "    temperature=0,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "# 流式输出\n",
    "for chunk in llm.stream(\"写一首关于秋天的诗歌\"):\n",
    "    print(chunk,end=\"\\n\",flush=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 追踪Token的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM文本类模型的token追踪\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "import os\n",
    "\n",
    "api_base = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "#构造一个llm\n",
    "llm = OpenAI(\n",
    "    openai_api_base=api_base,\n",
    "    openai_api_key=api_key,\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"给我讲一个笑话\")\n",
    "    print(result)\n",
    "    # 打印token使用情况\n",
    "    print(cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Models的token追踪\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "import os\n",
    "\n",
    "api_base = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=api_base,\n",
    "    openai_api_key=api_key,\n",
    "    model = \"gpt-4\",\n",
    "    temperature=0,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"给我讲一个笑话\")\n",
    "    print(result)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义输出\n",
    "\n",
    "- 输出函数参数\n",
    "- 输出json\n",
    "- 输出List\n",
    "- 输出日期\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讲笑话机器人：希望每次根据指令，可以输出一个这样的笑话(小明是怎么死的？笨死的)\n",
    "\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel,Field,validator\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "api_base = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "#构造LLM\n",
    "model = OpenAI(\n",
    "    openai_api_base=api_base,\n",
    "    openai_api_key=api_key,\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "#  定义个数据模型，用来描述最终的实例结构\n",
    "class Joke(BaseModel):\n",
    "    setup:str = Field(description=\"设置笑话的问题\")\n",
    "    punchline:str = Field(description=\"回答笑话的答案\")\n",
    "\n",
    "    # 验证问题是否符合要求\n",
    "    @validator(\"setup\")\n",
    "    def question_mark(cls,field):\n",
    "        if field[-1] != \"？\":\n",
    "            raise ValueError(\"不符合预期的问题格式!\")\n",
    "        return field\n",
    "\n",
    "# 将Joke数据模型传入\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"回答用户的输入.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 管道符调用,将prompt和model进行组合,prompt的结果作为model的输入\n",
    "prompt_and_model = prompt | model\n",
    "out_put = prompt_and_model.invoke({\"query\":\"给我讲一个笑话\"})\n",
    "print(\"out_put:\", out_put)\n",
    "# 调用parser对输出进行解析\n",
    "parser.invoke(out_put)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM的输出格式化成python list形式，类似['a','b','c']\n",
    "\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import OpenAI\n",
    "import os\n",
    "\n",
    "api_base = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "# 构造LLM\n",
    "model = OpenAI(\n",
    "    openai_api_base=api_base,\n",
    "    openai_api_key=api_key,\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"列出5个{subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "_input = prompt.format(subject=\"常见的小狗的名字\")\n",
    "output = model(_input)\n",
    "print(output)\n",
    "# 调用parser对输出进行解析\n",
    "parser.parse(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
