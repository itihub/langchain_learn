{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e22c87",
   "metadata": {},
   "source": [
    "# é“¾çš„é«˜çº§ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "764f8ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡\n",
    "load_dotenv(override=True)  # ä½¿ç”¨ override=True ç¡®ä¿åŠ è½½æœ€æ–°çš„ .env æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202ff3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.environ.get(\"OPENAPI_MODEL\"),\n",
    "    base_url=os.environ.get(\"OPENAPI_API_BASE\"),\n",
    "    api_key=os.environ.get(\"OPENAPI_API_KEY\"),\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf2fb4",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨@chainä¿®é¥°ç¬¦å¿«é€Ÿå°†å‡½æ•°å˜ä¸ºé“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2cd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The topic of this joke is a **pun** that plays on the double meaning of the words \"bare\" and \"bear.\"  \\n\\n- **Literal meaning**: Bears don\\'t wear shoes, so their feet are uncovered (*bare feet*).  \\n- **Pun twist**: Since they\\'re bears, their feet are also *bear feet*.  \\n\\nThe humor comes from the homophones (words that sound the same but have different meanings) \"bare\" and \"bear.\" The joke is lighthearted and centers on wordplay related to bears. ğŸ»ğŸ˜„'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# åˆ›å»ºç¬¬ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿ï¼šè¯·æ±‚å…³äºç‰¹å®šä¸»é¢˜çš„ç¬‘è¯\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "# åˆ›å»ºç¬¬äºŒä¸ªæç¤ºè¯æ¨¡æ¿ï¼šè¯¢é—®ç¬‘è¯çš„ä¸»é¢˜æ—¶ä»€ä¹ˆ\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the topic of this joke: {joke}?\")\n",
    "\n",
    "\n",
    "# ä½¿ç”¨@chainè£…é¥°å™¨å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰é“¾\n",
    "@chain\n",
    "def custom_chain(text):\n",
    "    # æ­¥éª¤1ï¼šå°†è¾“å…¥æ–‡æœ¬å¡«å……åˆ°ç¬¬ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿ä¸­\n",
    "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
    "    # æ­¥éª¤2ï¼šå°†ç¬¬ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿çš„è¾“å‡ºä¼ é€’ç»™è¯­è¨€æ¨¡å‹\n",
    "    output1 = llm.invoke(prompt_val1)\n",
    "    # æ­¥éª¤3ï¼šå°†è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œè§£æä¸ºå­—ç¬¦ä¸²\n",
    "    parsed_output1 = StrOutputParser().invoke(output1)\n",
    "\n",
    "    # æ­¥éª¤4ï¼šåˆ›å»ºç¬¬äºŒä¸ªå¤„ç†é“¾ï¼Œç”¨äºåˆ†æç¬‘è¯ä¸»é¢˜\n",
    "    # è¿™ä¸ªé“¾å°†æç¤ºè¯æ¨¡æ¿ã€æ¨¡å‹å’Œå­—ç¬¦ä¸²è§£æå™¨ä¸²è”èµ·æ¥\n",
    "    chain2 = prompt2 | llm | StrOutputParser()\n",
    "\n",
    "    # æ­¥éª¤5ï¼šå°†ç¬¬ä¸€ä¸ªé“¾çš„è¾“å‡ºä¼ é€’ç»™ç¬¬äºŒä¸ªé“¾\n",
    "    output2 = chain2.invoke({\"joke\": parsed_output1})\n",
    "    return output2\n",
    "\n",
    "\n",
    "# è°ƒç”¨è‡ªå®šä¹‰é“¾ï¼Œè¾“å…¥ä¸»é¢˜\"bears\"(ç†Š)\n",
    "# æ•´ä¸ªè¿‡ç¨‹ï¼š\n",
    "# 1. å…ˆç”Ÿæˆä¸€ä¸ªå…³äºç†Šçš„ç¬‘è¯\n",
    "# 2. ç„¶ååˆ†æè¿™ä¸ªç¬‘è¯çš„ä¸»é¢˜\n",
    "# 3. è¿”å›åˆ†æç»“æœ\n",
    "custom_chain.invoke(\"bears\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab32eeb",
   "metadata": {},
   "source": [
    "## åœ¨é“¾ä¸­ä½¿ç”¨lambdaå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79960c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To find the sum of **5** and **30**, follow these simple steps:\\n\\n1. **Identify the numbers to be added:**\\n   \\n   \\\\[\\n   5 \\\\quad \\\\text{and} \\\\quad 30\\n   \\\\]\\n\\n2. **Add the numbers together:**\\n   \\n   \\\\[\\n   5 + 30\\n   \\\\]\\n\\n3. **Calculate the sum:**\\n   \\n   \\\\[\\n   5 + 30 = 35\\n   \\\\]\\n\\n**Final Answer:**\\n\\n\\\\[\\n\\\\boxed{35}\\n\\\\]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 14, 'total_tokens': 123, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'deepseek-v3-250324', 'system_fingerprint': None, 'id': '021757570940697076ea4837f74e966daea642e85370365fa4f81', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4a19aa82-5266-4dd4-a6e0-cf21c6e90eed-0', usage_metadata={'input_tokens': 14, 'output_tokens': 109, 'total_tokens': 123, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# å®šä¹‰å‡½æ•°\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç®€å•çš„èŠå¤©æç¤ºæ¨¡æ¿ï¼Œè¯¢é—®aå’Œbçš„å’Œ\n",
    "prompt = ChatPromptTemplate.from_template(\"What is the sum of {a} and {b}?\")\n",
    "\n",
    "# æ„å»ºä¸€ä¸ªå¤æ‚çš„å¤„ç†é“¾\n",
    "chain = (\n",
    "    {\n",
    "        # å¤„ç†\"a\"å‚æ•°ï¼š\n",
    "        # 1. ä»è¾“å…¥å­—å…¸ä¸­æå–â€œfoo\"é”®çš„å€¼\n",
    "        # 2. å°†æå–çš„å€¼ä¼ é€’ç»™length_functionå‡½æ•°ï¼Œè®¡ç®—å…¶é•¿åº¦\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "        # å¤„ç†\"b\"å‚æ•°ï¼š\n",
    "        # 1. åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®å€¼å¯¹çš„å­—å…¸ï¼š\n",
    "        #   - \"text1\": ä»è¾“å…¥å­—å…¸ä¸­æå–â€œfoo\"é”®çš„å€¼\n",
    "        #   - \"text2\": ä»è¾“å…¥å­—å…¸ä¸­æå–â€œbar\"é”®çš„å€¼\n",
    "        # 2. å°†è¿™ä¸ªæ–°å­—å…¸ä¼ é€’ç»™multiple_length_functionå‡½æ•°\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt  # å°†å¤„ç†åçš„\"a\"å’Œ\"b\"ä¼ é€’ç»™æç¤ºæ¨¡æ¿\n",
    "    | llm  # å°†æç¤ºæ¨¡æ¿çš„è¾“å‡ºä¼ é€’ç»™è¯­è¨€æ¨¡å‹\n",
    ")\n",
    "\n",
    "# è°ƒç”¨é“¾å¤„ç†æµç¨‹ï¼Œè¾“å…¥ä¸€ä¸ªåŒ…å«\"foo\"å’Œ\"bar\"é”®çš„å­—å…¸\n",
    "chain.invoke({\"foo\": \"hello\", \"bar\": \"world!\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7305f3",
   "metadata": {},
   "source": [
    "## åœ¨é“¾ä¸­è‡ªå®šä¹‰æ”¯æŒæµè¾“å‡ºçš„å‡½æ•°\n",
    "+ å½“é“¾è¢«ä½¿ç”¨streamæˆ–astreamè°ƒç”¨çš„æ—¶å€™\n",
    "+ å¦‚æœåœ¨é“¾ä¸­å¢åŠ è‡ªå®šä¹‰å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eedd190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£•ç†Š,åŒ—æç†Š,ç†ŠçŒ«,é©¬æ¥ç†Š,çœ¼é•œç†Š"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªèŠå¤©æç¤ºæ¨¡æ¿ï¼Œè¦æ±‚ç”Ÿæˆ5ä¸ªä¸ç»™å®šåŠ¨ç‰©ç›¸ä¼¼çš„åŠ¨ç‰©åç§°ï¼Œä»¥é€—å·åˆ†éš”\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"è¯·åˆ—å‡º5ä¸ªä¸ä¸€ä¸‹åŠ¨ç‰©ç›¸ä¼¼çš„åŠ¨ç‰©åç§°ï¼Œç”¨é€—å·åˆ†éš”ï¼š{animal}ã€‚ä¸è¦åŒ…å«æ•°å­—\"\n",
    ")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªå¤„ç†é“¾ï¼šæç¤ºè¯æ¨¡æ¿ -> è¯­è¨€æ¨¡å‹ -> å­—ç¬¦ä¸²è§£æå™¨\n",
    "str_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# æµå¼è¾“å‡ºç»“æœï¼Œè¾“å…¥ä¸ºâ€œç†Šâ€\n",
    "for chunk in str_chain.stream({\"animal\": \"ç†Š\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f781adae",
   "metadata": {},
   "source": [
    "### å¢åŠ è‡ªå®šä¹‰å‡½æ•°\n",
    "+ èšåˆå½“å‰æµä¼ è¾“çš„è¾“å‡º\n",
    "+ åœ¨ç”Ÿäº§ä¸‹ä¸€ä¸ªé€—å·çš„æ—¶å€™ç»„åˆ\n",
    "+ æ³¨æ„ï¼šä½¿ç”¨yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e0f460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['æ£•ç†Š']['åŒ—æç†Š']['ç†ŠçŒ«']['é©¬æ¥ç†Š']['çœ¼é•œç†Š']"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰è‡ªå®šä¹‰è§£æå™¨ï¼Œå°†LLMè¾“å‡ºçš„æ ‡è®°è¿­ä»£å™¨\n",
    "# æŒ‰é€—å·åˆ†éš”è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨\n",
    "from typing import Iterator, List\n",
    "\n",
    "\n",
    "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n",
    "    # ä¿å­˜éƒ¨åˆ†è¾“å…¥ç›´åˆ°é‡åˆ°é€—å·\n",
    "    buffer = \"\"\n",
    "    for chunk in input:\n",
    "        # å°†å½“å‰å—æ·»åŠ åˆ°ç¼“å†²åŒº\n",
    "        buffer += chunk\n",
    "        # å½“ç¼“å­˜åŒºä¸­æœ‰é€—å·æ—¶\n",
    "        while \",\" in buffer:\n",
    "            # åˆ†å‰²ç¼“å­˜åŒºï¼Œé€—å·å‰çš„éƒ¨åˆ†ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„è¾“å‡º\n",
    "            comma_index = buffer.index(\",\")\n",
    "            # è¾“å‡ºé€—å·ä¹‹å‰çš„æ‰€æœ‰å†…å®¹\n",
    "            yield [buffer[:comma_index].strip()]\n",
    "            # ä¿å­˜å‰©ä½™éƒ¨åˆ†ç”¨äºä¸‹ä¸€æ¬¡è¿­ä»£\n",
    "            buffer = buffer[comma_index + 1 :]\n",
    "\n",
    "    # è¾“å‡ºæœ€åå‰©ä½™çš„éƒ¨åˆ†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰\n",
    "    yield [buffer.strip()]\n",
    "\n",
    "\n",
    "list_chain = str_chain | split_into_list\n",
    "\n",
    "for chunk in list_chain.stream({\"animal\": \"ç†Š\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f2933",
   "metadata": {},
   "source": [
    "### yield ä¸ return åŒºåˆ«\n",
    "+ returnå‡½æ•°ç«‹å³è®¡ç®—å¹¶è¿”å›æ‰€æœ‰ç»“æœï¼Œè€Œyieldå‡½æ•°æŒ‰éœ€è®¡ç®—ç»“æœ\n",
    "+ returnå‡½æ•°è¿”å›ä¸€ä¸ªæ•°æ®ç»“æ„ï¼ˆå¦‚åˆ—è¡¨ï¼‰ï¼Œyieldå‡½æ•°è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡\n",
    "+ yieldå‡½æ•°å¯ä»¥å¤„ç†æ½œåœ¨çš„æ— é™åºåˆ—ï¼Œè€Œreturnå‡½æ•°å¿…é¡»åœ¨æœ‰é™æ—¶é—´å†…å®Œæˆ\n",
    "+ ç”Ÿæˆå™¨å¯¹è±¡æ˜¯ä¸€æ¬¡æ€§çš„ï¼Œéå†å®Œåå°±è¢«æ¶ˆè€—å®Œæ¯•ï¼Œè€Œreturnè¿”å›çš„æ•°æ®ç»“æ„å¯ä»¥é‡å¤ä½¿ç”¨\n",
    "+ yield ç‰¹åˆ«é€‚åˆå¤„ç†å¤§æ•°æ®é›†æˆ–æµå¼æ•°æ®ï¼Œå› ä¸ºå®ƒä¸éœ€è¦ä¸€æ¬¡æ€§å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å®¹ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd37593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨ return å‡½æ•°: [0, 1, 4, 9, 16]\n",
      "ç±»å‹ï¼š <class 'list'>\n",
      "éå†ç»“æœ: 0\n",
      "éå†ç»“æœ: 1\n",
      "éå†ç»“æœ: 4\n",
      "éå†ç»“æœ: 9\n",
      "éå†ç»“æœ: 16\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨return\n",
    "def get_squares_return(n):\n",
    "    \"\"\"è¿”å›åŒ…å«0åˆ°n-1çš„å¹³æ–¹çš„åˆ—è¡¨\"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(i * i)\n",
    "    return result  # ä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰ç»“æœ\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ return å‡½æ•°\n",
    "squares = get_squares_return(5)\n",
    "print(\"ä½¿ç”¨ return å‡½æ•°:\", squares)  # è¾“å‡º: [0, 1, 4, 9, 16]\n",
    "print(\"è¿”å›ç±»å‹ï¼š\", type(squares))  # è¾“å‡º: <class 'list'>\n",
    "\n",
    "# éå†ç»“æœ\n",
    "for num in squares:\n",
    "    print(\"éå†ç»“æœ:\", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7635226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨ yield å‡½æ•°: <generator object get_squares_yield at 0x7c128db31cb0>\n",
      "è¿”å›ç±»å‹ï¼š <class 'generator'>\n",
      "éå†ç»“æœ: 0\n",
      "éå†ç»“æœ: 1\n",
      "éå†ç»“æœ: 4\n",
      "éå†ç»“æœ: 9\n",
      "éå†ç»“æœ: 16\n",
      "å†æ¬¡éå†\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ yield\n",
    "def get_squares_yield(n):\n",
    "    \"\"\"ç”ŸæˆåŒ…å«0åˆ°n-1çš„å¹³æ–¹çš„å€¼\"\"\"\n",
    "    for i in range(n):\n",
    "        yield i * i  # æŒ‰éœ€ç”Ÿæˆç»“æœ\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ yield å‡½æ•°\n",
    "squares_gen = get_squares_yield(5)\n",
    "print(\n",
    "    \"ä½¿ç”¨ yield å‡½æ•°:\", squares_gen\n",
    ")  # è¾“å‡º: <generator object get_squares_yield at 0x...>\n",
    "print(\"è¿”å›ç±»å‹ï¼š\", type(squares_gen))  # è¾“å‡º: <class 'generator'>\n",
    "\n",
    "# éå†ç»“æœ\n",
    "for num in squares_gen:\n",
    "    print(\"éå†ç»“æœ:\", num)\n",
    "\n",
    "# å†æ¬¡éå†ç”Ÿæˆå™¨\n",
    "print(\"å†æ¬¡éå†\")\n",
    "for num in squares_gen:\n",
    "    print(\"å†æ¬¡éå†ç»“æœ:\", num)  # ä¸ä¼šè¾“å‡ºä»»ä½•å†…å®¹ï¼Œå› ä¸ºç”Ÿæˆå™¨å·²ç»è¢«æ¶ˆè€—å®Œæ¯•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e2006",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨RunnablePassthroughæ¥ä¼ é€’å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010c1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'modified': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªå¯ä»¥å¹¶è¡Œè¿è¡Œçš„å¤„ç†æµç¨‹\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),  # ç¬¬ä¸€ä¸ªå¤„ç†å™¨ï¼šç›´æ¥ä¼ å…¥è¾“å…¥ï¼Œä¸åšä¿®æ”¹\n",
    "    modified=lambda x: x[\"num\"] + 1,  # ç¬¬äºŒä¸ªå¤„ç†å™¨ï¼šå–å‡ºè¾“å…¥ä¸­çš„â€œnumâ€å€¼å¹¶+1\n",
    ")\n",
    "\n",
    "# æ‰§è¡Œè¿™ä¸ªå¤„ç†æµç¨‹ï¼Œè¾“å…¥æ—¶ä¸€ä¸ªåŒ…å«â€œnumâ€å­—æ®µçš„å­—å…¸\n",
    "runnable.invoke({\"num\": 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9696a",
   "metadata": {},
   "source": [
    "## LCELæ”¯æŒåœ¨è¿è¡Œæ—¶å€™å¯¹é“¾è¿›è¡Œé…ç½®\n",
    "+ åŠ¨æ€æ”¹å†™æ¨¡å‹çš„æ¸©åº¦\n",
    "+ åŠ¨æ€åˆ‡æ¢æç¤ºè¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f21848",
   "metadata": {},
   "source": [
    "### åŠ¨æ€æ”¹å†™æ¨¡å‹æ¸©åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53bc3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½çš„ï¼Œæˆ‘å°†éšæœºæŒ‘é€‰ä¸€ä¸ªæ•´æ•°ã€‚ä»¥ä¸‹æ˜¯ç»“æœï¼š\n",
      "\n",
      "**éšæœºæ•´æ•°ï¼š** 42\n",
      "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªéšæœºæŒ‘é€‰çš„æ•´æ•°ï¼š\n",
      "\n",
      "**42**  \n",
      "\n",
      "ï¼ˆ*æ³¨ï¼šè™½ç„¶è¿™ä¸ªæ•°å­—çœ‹èµ·æ¥åƒæ˜¯â€œç”Ÿå‘½ã€å®‡å®™ä»¥åŠä»»ä½•äº‹æƒ…çš„ç»ˆæç­”æ¡ˆâ€ï¼Œä½†å®ƒç¡®å®æ˜¯éšæœºç”Ÿæˆçš„ï¼å¦‚æœéœ€è¦å…¶ä»–éšæœºæ•°ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘èŒƒå›´æˆ–æ•°é‡å“¦~*ï¼‰\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.environ.get(\"OPENAPI_MODEL\"),\n",
    "    base_url=os.environ.get(\"OPENAPI_API_BASE\"),\n",
    "    api_key=os.environ.get(\"OPENAPI_API_KEY\"),\n",
    "    temperature=0,\n",
    ").configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM Temperature\",\n",
    "        description=\"Temperature for response randomness\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# æœªä¿®æ”¹temperature\n",
    "response1 = llm.invoke(\"éšæ„æŒ‘é€‰ä¸€ä¸ªéšæœºæ•°ï¼Œè¾“å‡ºä¸ºä¸€ä¸ªæ•´æ•°\")\n",
    "print(response1.content)\n",
    "\n",
    "# åŠ¨æ€ä¿®æ”¹temperature\n",
    "response2 = llm.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\n",
    "    \"éšæ„æŒ‘é€‰ä¸€ä¸ªéšæœºæ•°ï¼Œè¾“å‡ºä¸ºä¸€ä¸ªæ•´æ•°\"\n",
    ")\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8def4",
   "metadata": {},
   "source": [
    "### é“¾çš„æç¤ºè¯åŠ¨æ€åˆ‡æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "681a4876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: foo \\nContext: bar \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n",
      "messages=[HumanMessage(content=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: foo \\nContext: bar \\nAnswer: [/INST]\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.runnables.hub import HubRunnable\n",
    "\n",
    "prompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(\n",
    "    owner_repo_commit=ConfigurableField(\n",
    "        id=\"hub_commit\",\n",
    "        name=\"Hub Commit\",\n",
    "        description=\"The Hub commit to pull from\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# æœªåˆ‡æ¢æç¤ºè¯\n",
    "prompt_value1 = prompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})\n",
    "print(prompt_value1)\n",
    "\n",
    "# åŠ¨æ€åˆ‡æ¢æç¤ºè¯\n",
    "prompt_value2 = prompt.with_config(\n",
    "    configurable={\"hub_commit\": \"rlm/rag-prompt-llama\"}\n",
    ").invoke({\"question\": \"foo\", \"context\": \"bar\"})\n",
    "print(prompt_value2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf37319",
   "metadata": {},
   "source": [
    "## ä¸ºé“¾æ·»åŠ è®°å¿†èƒ½åŠ›\n",
    "+ æ³¨æ„ï¼šç®€å•çš„é“¾çš„è®°å¿†æ·»åŠ å¯ä»¥ä½¿ç”¨v0.2æ–¹å¼ï¼Œå¤æ‚çš„è®°å¿†å®˜æ–¹æ¨èä½¿ç”¨LangGraph\n",
    "+ çŸ­æ—¶è®°å¿†ï¼šInMemoryHistory\n",
    "+ é•¿æ—¶è®°å¿†ï¼šRunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58972534",
   "metadata": {},
   "source": [
    "### çŸ­æ—¶è®°å¿†ï¼šInMemoryHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40a4f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': InMemoryHistory(messages=[AIMessage(content='ä½ å¥½', additional_kwargs={}, response_metadata={})])}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"å†…å­˜ä¸­å®ç°çš„èŠå¤©æ¶ˆæ¯å†å²è®°å½•.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(\n",
    "        default_factory=list\n",
    "    )  # ä½¿ç”¨ç©ºåˆ—è¡¨ä½œä¸ºé»˜è®¤å€¼å­˜å‚¨æ¶ˆæ¯\n",
    "\n",
    "    def add_message(self, message: List[BaseMessage]) -> None:\n",
    "        \"\"\"æ·»åŠ ä¸€ç»„æ¶ˆæ¯åˆ°å­˜å‚¨ä¸­.\"\"\"\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "# è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å…¨å±€éå†æ¥å­˜å‚¨èŠå¤©æ¶ˆæ¯å†å²\n",
    "# è¿™æ ·å¯ä»¥æ›´å®¹æ˜“åœ°æ£€æŸ¥å®ƒä»¥æŸ¥çœ‹åº•å±‚ç»“æœ\n",
    "store = {}  # åˆ›å»ºç©ºå­—å…¸ç”¨äºå­˜å‚¨ä¸åŒä¼šè¯çš„å†å²è®°å½•\n",
    "\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    æ ¹æ®ä¼šè¯IDè·å–å†å²è®°å½•.\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()  # ä¸ºæ–°ä¼šè¯åˆ›å»ºæ–°çš„å†å²è®°å½•å¯¹è±¡\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# è·å–ä¼šè¯IDä¸ºâ€œ1â€çš„å†å²è®°å½•\n",
    "history = get_by_session_id(\"1\")\n",
    "# æ·»åŠ ä¸€æ¡AIæ¶ˆæ¯åˆ°å†å²è®°å½•\n",
    "history.add_message(AIMessage(content=\"ä½ å¥½\"))\n",
    "# æ‰“å°å­˜å‚¨çš„æ‰€æœ‰å†å²è®°å½•\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7300885",
   "metadata": {},
   "source": [
    "### åœ¨é“¾ä¸­å¢åŠ çŸ­æœŸè®°å¿†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cae5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ä½™å¼¦ï¼ˆcosineï¼‰æ˜¯ä¸‰è§’å‡½æ•°ä¸­çš„ä¸€ç§åŸºæœ¬å‡½æ•°ï¼Œé€šå¸¸è¡¨ç¤ºä¸º \\\\(\\\\cos \\\\theta\\\\)ï¼Œå…¶ä¸­ \\\\(\\\\theta\\\\) æ˜¯ä¸€ä¸ªè§’åº¦ã€‚ä½™å¼¦å‡½æ•°çš„å®šä¹‰å’Œå«ä¹‰å¯ä»¥ä»å¤šä¸ªè§’åº¦ç†è§£ï¼š\\n\\n### 1. **ç›´è§’ä¸‰è§’å½¢ä¸­çš„å®šä¹‰**\\nåœ¨ç›´è§’ä¸‰è§’å½¢ä¸­ï¼Œä½™å¼¦è¡¨ç¤º**é‚»è¾¹ä¸æ–œè¾¹çš„æ¯”å€¼**ï¼š\\n\\\\[\\n\\\\cos \\\\theta = \\\\frac{\\\\text{é‚»è¾¹}}{\\\\text{æ–œè¾¹}}\\n\\\\]\\nä¾‹å¦‚ï¼Œè‹¥è§’ \\\\(\\\\theta\\\\) çš„é‚»è¾¹é•¿åº¦ä¸º \\\\(a\\\\)ï¼Œæ–œè¾¹é•¿åº¦ä¸º \\\\(c\\\\)ï¼Œåˆ™ \\\\(\\\\cos \\\\theta = \\\\frac{a}{c}\\\\)ã€‚\\n\\n### 2. **å•ä½åœ†ä¸Šçš„å®šä¹‰**\\nåœ¨ç›´è§’åæ ‡ç³»ä¸­ï¼Œä»¥åŸç‚¹ä¸ºä¸­å¿ƒã€åŠå¾„ä¸º1çš„å•ä½åœ†ä¸Šï¼Œä»»æ„è§’åº¦ \\\\(\\\\theta\\\\) å¯¹åº”çš„ç»ˆè¾¹ä¸å•ä½åœ†çš„äº¤ç‚¹åæ ‡ä¸º \\\\((\\\\cos \\\\theta, \\\\sin \\\\theta)\\\\)ã€‚æ­¤æ—¶ï¼š\\n- **ä½™å¼¦å€¼**æ˜¯è¯¥ç‚¹çš„æ¨ªåæ ‡ï¼ˆ\\\\(x\\\\) åæ ‡ï¼‰ã€‚\\n- å½“è§’åº¦ \\\\(\\\\theta\\\\) å˜åŒ–æ—¶ï¼Œ\\\\(\\\\cos \\\\theta\\\\) çš„å€¼åœ¨ \\\\([-1, 1]\\\\) ä¹‹é—´å‘¨æœŸæ€§æ³¢åŠ¨ã€‚\\n\\n### 3. **å‘¨æœŸæ€§å‡½æ•°**\\nä½™å¼¦å‡½æ•°æ˜¯å‘¨æœŸä¸º \\\\(2\\\\pi\\\\)ï¼ˆæˆ– \\\\(360^\\\\circ\\\\)ï¼‰çš„å‘¨æœŸå‡½æ•°ï¼Œæ»¡è¶³ï¼š\\n\\\\[\\n\\\\cos (\\\\theta + 2\\\\pi) = \\\\cos \\\\theta\\n\\\\]\\nå…¶å›¾åƒä¸ºä¸€æ¡è¿ç»­çš„æ³¢æµªçº¿ï¼ˆä½™å¼¦æ›²çº¿ï¼‰ï¼Œå…³äº \\\\(y\\\\) è½´å¯¹ç§°ï¼ˆå¶å‡½æ•°ï¼‰ã€‚\\n\\n### 4. **çº§æ•°å±•å¼€**\\nä½™å¼¦å¯ä»¥é€šè¿‡æ— é™çº§æ•°ï¼ˆæ³°å‹’çº§æ•°ï¼‰è¡¨ç¤ºï¼š\\n\\\\[\\n\\\\cos \\\\theta = 1 - \\\\frac{\\\\theta^2}{2!} + \\\\frac{\\\\theta^4}{4!} - \\\\frac{\\\\theta^6}{6!} + \\\\cdots \\\\quad (\\\\theta \\\\text{ä¸ºå¼§åº¦})\\n\\\\]\\n\\n### 5. **åº”ç”¨é¢†åŸŸ**\\n- **å‡ ä½•å­¦**ï¼šè®¡ç®—è§’åº¦æˆ–è¾¹é•¿ã€‚\\n- **ç‰©ç†å­¦**ï¼šæè¿°æŒ¯åŠ¨ã€æ³¢åŠ¨ï¼ˆå¦‚ç®€è°è¿åŠ¨ï¼‰ã€‚\\n- **å·¥ç¨‹å­¦**ï¼šä¿¡å·å¤„ç†ã€å‚…é‡Œå¶åˆ†æã€‚\\n- **è®¡ç®—æœºå›¾å½¢å­¦**ï¼šæ—‹è½¬å’Œåæ ‡å˜æ¢ã€‚\\n\\n### ç¤ºä¾‹\\nè‹¥ \\\\(\\\\theta = 60^\\\\circ\\\\)ï¼Œåˆ™ \\\\(\\\\cos 60^\\\\circ = 0.5\\\\)ï¼ˆå³é‚»è¾¹ä¸ºæ–œè¾¹çš„ä¸€åŠï¼‰ã€‚\\n\\næ€»ç»“æ¥è¯´ï¼Œä½™å¼¦æ˜¯æè¿°è§’åº¦ä¸ç›´è§’ä¸‰è§’å½¢è¾¹é•¿å…³ç³»æˆ–å•ä½åœ†ä¸Šåæ ‡å…³ç³»çš„å‡½æ•°ï¼Œå¹¿æ³›åº”ç”¨äºæ•°å­¦å’Œç§‘å­¦é¢†åŸŸã€‚' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 486, 'prompt_tokens': 15, 'total_tokens': 501, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'deepseek-v3-250324', 'system_fingerprint': None, 'id': '021757576121871557788d0d3fd642eba8bd023171b2dbe5f9407', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--a441c0e1-a2ee-422e-847d-96c2e5352288-0' usage_metadata={'input_tokens': 15, 'output_tokens': 486, 'total_tokens': 501, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
      "{'1': InMemoryHistory(messages=[AIMessage(content='ä½ å¥½', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½™å¼¦çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='ä½™å¼¦ï¼ˆcosineï¼‰æ˜¯ä¸‰è§’å‡½æ•°ä¸­çš„ä¸€ç§åŸºæœ¬å‡½æ•°ï¼Œé€šå¸¸è¡¨ç¤ºä¸º \\\\(\\\\cos \\\\theta\\\\)ï¼Œå…¶ä¸­ \\\\(\\\\theta\\\\) æ˜¯ä¸€ä¸ªè§’åº¦ã€‚ä½™å¼¦å‡½æ•°çš„å®šä¹‰å’Œå«ä¹‰å¯ä»¥ä»å¤šä¸ªè§’åº¦ç†è§£ï¼š\\n\\n### 1. **ç›´è§’ä¸‰è§’å½¢ä¸­çš„å®šä¹‰**\\nåœ¨ç›´è§’ä¸‰è§’å½¢ä¸­ï¼Œä½™å¼¦è¡¨ç¤º**é‚»è¾¹ä¸æ–œè¾¹çš„æ¯”å€¼**ï¼š\\n\\\\[\\n\\\\cos \\\\theta = \\\\frac{\\\\text{é‚»è¾¹}}{\\\\text{æ–œè¾¹}}\\n\\\\]\\nä¾‹å¦‚ï¼Œè‹¥è§’ \\\\(\\\\theta\\\\) çš„é‚»è¾¹é•¿åº¦ä¸º \\\\(a\\\\)ï¼Œæ–œè¾¹é•¿åº¦ä¸º \\\\(c\\\\)ï¼Œåˆ™ \\\\(\\\\cos \\\\theta = \\\\frac{a}{c}\\\\)ã€‚\\n\\n### 2. **å•ä½åœ†ä¸Šçš„å®šä¹‰**\\nåœ¨ç›´è§’åæ ‡ç³»ä¸­ï¼Œä»¥åŸç‚¹ä¸ºä¸­å¿ƒã€åŠå¾„ä¸º1çš„å•ä½åœ†ä¸Šï¼Œä»»æ„è§’åº¦ \\\\(\\\\theta\\\\) å¯¹åº”çš„ç»ˆè¾¹ä¸å•ä½åœ†çš„äº¤ç‚¹åæ ‡ä¸º \\\\((\\\\cos \\\\theta, \\\\sin \\\\theta)\\\\)ã€‚æ­¤æ—¶ï¼š\\n- **ä½™å¼¦å€¼**æ˜¯è¯¥ç‚¹çš„æ¨ªåæ ‡ï¼ˆ\\\\(x\\\\) åæ ‡ï¼‰ã€‚\\n- å½“è§’åº¦ \\\\(\\\\theta\\\\) å˜åŒ–æ—¶ï¼Œ\\\\(\\\\cos \\\\theta\\\\) çš„å€¼åœ¨ \\\\([-1, 1]\\\\) ä¹‹é—´å‘¨æœŸæ€§æ³¢åŠ¨ã€‚\\n\\n### 3. **å‘¨æœŸæ€§å‡½æ•°**\\nä½™å¼¦å‡½æ•°æ˜¯å‘¨æœŸä¸º \\\\(2\\\\pi\\\\)ï¼ˆæˆ– \\\\(360^\\\\circ\\\\)ï¼‰çš„å‘¨æœŸå‡½æ•°ï¼Œæ»¡è¶³ï¼š\\n\\\\[\\n\\\\cos (\\\\theta + 2\\\\pi) = \\\\cos \\\\theta\\n\\\\]\\nå…¶å›¾åƒä¸ºä¸€æ¡è¿ç»­çš„æ³¢æµªçº¿ï¼ˆä½™å¼¦æ›²çº¿ï¼‰ï¼Œå…³äº \\\\(y\\\\) è½´å¯¹ç§°ï¼ˆå¶å‡½æ•°ï¼‰ã€‚\\n\\n### 4. **çº§æ•°å±•å¼€**\\nä½™å¼¦å¯ä»¥é€šè¿‡æ— é™çº§æ•°ï¼ˆæ³°å‹’çº§æ•°ï¼‰è¡¨ç¤ºï¼š\\n\\\\[\\n\\\\cos \\\\theta = 1 - \\\\frac{\\\\theta^2}{2!} + \\\\frac{\\\\theta^4}{4!} - \\\\frac{\\\\theta^6}{6!} + \\\\cdots \\\\quad (\\\\theta \\\\text{ä¸ºå¼§åº¦})\\n\\\\]\\n\\n### 5. **åº”ç”¨é¢†åŸŸ**\\n- **å‡ ä½•å­¦**ï¼šè®¡ç®—è§’åº¦æˆ–è¾¹é•¿ã€‚\\n- **ç‰©ç†å­¦**ï¼šæè¿°æŒ¯åŠ¨ã€æ³¢åŠ¨ï¼ˆå¦‚ç®€è°è¿åŠ¨ï¼‰ã€‚\\n- **å·¥ç¨‹å­¦**ï¼šä¿¡å·å¤„ç†ã€å‚…é‡Œå¶åˆ†æã€‚\\n- **è®¡ç®—æœºå›¾å½¢å­¦**ï¼šæ—‹è½¬å’Œåæ ‡å˜æ¢ã€‚\\n\\n### ç¤ºä¾‹\\nè‹¥ \\\\(\\\\theta = 60^\\\\circ\\\\)ï¼Œåˆ™ \\\\(\\\\cos 60^\\\\circ = 0.5\\\\)ï¼ˆå³é‚»è¾¹ä¸ºæ–œè¾¹çš„ä¸€åŠï¼‰ã€‚\\n\\næ€»ç»“æ¥è¯´ï¼Œä½™å¼¦æ˜¯æè¿°è§’åº¦ä¸ç›´è§’ä¸‰è§’å½¢è¾¹é•¿å…³ç³»æˆ–å•ä½åœ†ä¸Šåæ ‡å…³ç³»çš„å‡½æ•°ï¼Œå¹¿æ³›åº”ç”¨äºæ•°å­¦å’Œç§‘å­¦é¢†åŸŸã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 486, 'prompt_tokens': 15, 'total_tokens': 501, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'deepseek-v3-250324', 'system_fingerprint': None, 'id': '021757576121871557788d0d3fd642eba8bd023171b2dbe5f9407', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a441c0e1-a2ee-422e-847d-96c2e5352288-0', usage_metadata={'input_tokens': 15, 'output_tokens': 486, 'total_tokens': 501, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})])}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.environ.get(\"OPENAPI_MODEL\"),\n",
    "    base_url=os.environ.get(\"OPENAPI_API_BASE\"),\n",
    "    api_key=os.environ.get(\"OPENAPI_API_KEY\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºï¼Œå†å²è®°å½•å’Œç”¨æˆ·é—®é¢˜\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ æ˜¯ä¸€ä¸ªæ“…é•¿{ability}çš„åŠ©æ‰‹\",\n",
    "        ),  # ç³»ç»Ÿè§’è‰²æç¤ºï¼Œä½¿ç”¨abilityéå†å®šä¹‰åŠ©æ‰‹ä¸“é•¿\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # æ”¾ç½®å†å²æ¶ˆæ¯çš„å ä½ç¬¦\n",
    "        (\"human\", \"{input}\"),  # ç”¨æˆ·é—®é¢˜çš„å ä½ç¬¦\n",
    "    ]\n",
    ")\n",
    "\n",
    "# å°†æç¤ºè¯æ¨¡æ¿å’Œå¤§æ¨¡å‹è¿æ¥æˆä¸€ä¸ªé“¾\n",
    "chain = prompt | llm\n",
    "\n",
    "# åˆ›å»ºå¸¦æœ‰æ¶ˆæ¯å†å²åŠŸèƒ½çš„å¯è¿è¡Œé“¾\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_by_session_id,  # è·å–å†å²è®°å½•çš„å‡½æ•°\n",
    "    input_messages_key=\"input\",  # ç”¨æˆ·è¾“å…¥çš„é”®å\n",
    "    history_messages_key=\"history\",  # å†å²æ¶ˆæ¯çš„é”®å\n",
    ")\n",
    "\n",
    "# é¦–æ¬¡è°ƒç”¨é“¾ï¼Œè¯¢é—®ä½™å¼¦çš„å«ä¹‰\n",
    "response = chain_with_history.invoke(\n",
    "    {\"ability\": \"æ•°å­¦\", \"input\": \"ä½™å¼¦çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\"},\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"session_id\": \"1\",  # é…ç½®ä¼šè¯ID\n",
    "        }\n",
    "    },\n",
    ")\n",
    "print(response)\n",
    "\n",
    "# æ‰“å°å­˜å‚¨ä¸­çš„å†å²è®°å½•\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ae19318",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.clear()  # æ¸…ç©ºå­˜å‚¨ä»¥ä¾¿ä¸‹æ¬¡æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00d24184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ä½™å¼¦ï¼ˆcosineï¼‰æ˜¯ä¸‰è§’å‡½æ•°ä¹‹ä¸€ï¼Œé€šå¸¸ç”¨äºæè¿°ç›´è§’ä¸‰è§’å½¢ä¸­è§’åº¦ä¸è¾¹é•¿ä¹‹é—´çš„å…³ç³»ï¼Œæˆ–åœ¨å•ä½åœ†ä¸­è¡¨ç¤ºè§’åº¦ä¸åæ ‡çš„å¯¹åº”å…³ç³»ã€‚ä»¥ä¸‹æ˜¯å…¶æ ¸å¿ƒå«ä¹‰å’Œå®šä¹‰ï¼š\\n\\n### 1. **ç›´è§’ä¸‰è§’å½¢ä¸­çš„å®šä¹‰**\\nåœ¨ç›´è§’ä¸‰è§’å½¢ä¸­ï¼Œä½™å¼¦å€¼å®šä¹‰ä¸ºæŸä¸€é”è§’çš„é‚»è¾¹é•¿åº¦ä¸æ–œè¾¹é•¿åº¦çš„æ¯”å€¼ï¼š\\n\\\\[\\n\\\\cos \\\\theta = \\\\frac{\\\\text{é‚»è¾¹}}{\\\\text{æ–œè¾¹}}\\n\\\\]\\nä¾‹å¦‚ï¼Œè‹¥è§’Î¸çš„é‚»è¾¹é•¿ä¸º3ï¼Œæ–œè¾¹ä¸º5ï¼Œåˆ™ \\\\(\\\\cos \\\\theta = \\\\frac{3}{5}\\\\)ã€‚\\n\\n### 2. **å•ä½åœ†ä¸­çš„å®šä¹‰**\\nåœ¨ç›´è§’åæ ‡ç³»ä¸­ï¼Œä»¥åŸç‚¹ä¸ºä¸­å¿ƒã€åŠå¾„ä¸º1çš„å•ä½åœ†ä¸Šï¼Œä»»æ„è§’åº¦Î¸å¯¹åº”çš„ç»ˆè¾¹ä¸åœ†äº¤äºç‚¹ \\\\((x, y)\\\\)ï¼Œåˆ™ï¼š\\n\\\\[\\n\\\\cos \\\\theta = x \\\\text{ï¼ˆæ¨ªåæ ‡ï¼‰}\\n\\\\]\\næ­¤æ—¶ï¼Œä½™å¼¦å€¼ç›´æ¥åæ˜ è§’åº¦Î¸åœ¨å•ä½åœ†ä¸Šçš„æ°´å¹³æŠ•å½±ã€‚\\n\\n### 3. **å‘¨æœŸæ€§å‡½æ•°**\\nä½™å¼¦å‡½æ•°æ˜¯å‘¨æœŸä¸º \\\\(2\\\\pi\\\\)ï¼ˆ360Â°ï¼‰çš„å‘¨æœŸå‡½æ•°ï¼Œæ»¡è¶³ï¼š\\n\\\\[\\n\\\\cos(\\\\theta + 2\\\\pi) = \\\\cos \\\\theta\\n\\\\]\\nå…¶å›¾åƒä¸ºè¿ç»­çš„æ³¢æµªå½¢æ›²çº¿ï¼ˆä½™å¼¦æ³¢ï¼‰ï¼Œåœ¨ \\\\([0, \\\\pi]\\\\) å•è°ƒé€’å‡ï¼Œå€¼åŸŸä¸º \\\\([-1, 1]\\\\)ã€‚\\n\\n### 4. **æ‰©å±•å®šä¹‰ï¼ˆä»»æ„è§’ï¼‰**\\né€šè¿‡å•ä½åœ†å®šä¹‰ï¼Œä½™å¼¦å¯æ¨å¹¿åˆ°ä»»æ„è§’åº¦ï¼ˆåŒ…æ‹¬è´Ÿè§’å’Œå¤§äº360Â°çš„è§’ï¼‰ã€‚ä¾‹å¦‚ï¼š\\n- \\\\(\\\\cos 0Â° = 1\\\\)ï¼ˆç»ˆè¾¹ä¸xè½´æ­£æ–¹å‘é‡åˆï¼‰\\n- \\\\(\\\\cos 90Â° = 0\\\\)ï¼ˆç»ˆè¾¹ä¸yè½´æ­£æ–¹å‘é‡åˆï¼‰\\n- \\\\(\\\\cos 180Â° = -1\\\\)\\n\\n### 5. **ä¸å…¶ä»–å‡½æ•°çš„å…³ç³»**\\n- **ä¸æ­£å¼¦çš„å…³ç³»**ï¼š\\\\(\\\\cos \\\\theta = \\\\sin\\\\left(\\\\frac{\\\\pi}{2} - \\\\theta\\\\right)\\\\)\\n- **å‹¾è‚¡å®šç†**ï¼š\\\\(\\\\sin^2 \\\\theta + \\\\cos^2 \\\\theta = 1\\\\)\\n- **å¯¼æ•°**ï¼š\\\\(\\\\frac{d}{d\\\\theta} \\\\cos \\\\theta = -\\\\sin \\\\theta\\\\)\\n\\n### 6. **åº”ç”¨åœºæ™¯**\\n- **ç‰©ç†å­¦**ï¼šæè¿°ç®€è°æŒ¯åŠ¨ã€æ³¢åŠ¨ç°è±¡ã€‚\\n- **å·¥ç¨‹å­¦**ï¼šè®¡ç®—åŠ›çš„åˆ†è§£ã€äº¤æµç”µåˆ†æã€‚\\n- **è®¡ç®—æœºå›¾å½¢å­¦**ï¼šæ—‹è½¬çŸ©é˜µã€åæ ‡å˜æ¢ã€‚\\n\\n### ç¤ºä¾‹è®¡ç®—\\næ±‚ \\\\(60Â°\\\\) çš„ä½™å¼¦å€¼ï¼š\\n- **å•ä½åœ†**ï¼šç»ˆè¾¹ä¸åœ†äº¤äº \\\\((\\\\frac{1}{2}, \\\\frac{\\\\sqrt{3}}{2})\\\\)ï¼Œæ•… \\\\(\\\\cos 60Â° = \\\\frac{1}{2}\\\\)ã€‚\\n- **ç‰¹æ®Šè§’è®°å¿†**ï¼šå¸¸ç”¨è§’ï¼ˆå¦‚30Â°, 45Â°, 60Â°ï¼‰çš„ä½™å¼¦å€¼å¯é€šè¿‡ä¸‰è§’å½¢æ¯”ä¾‹æˆ–å•ä½åœ†å¿«é€Ÿå¾—å‡ºã€‚\\n\\næ€»ç»“æ¥è¯´ï¼Œä½™å¼¦ä»å‡ ä½•ä¸Šæ­ç¤ºäº†è§’åº¦ä¸è¾¹é•¿æˆ–åæ ‡çš„å…³ç³»ï¼Œå¹¶åœ¨æ•°å­¦å’Œç§‘å­¦ä¸­å¹¿æ³›åº”ç”¨äºå‘¨æœŸæ€§ç°è±¡çš„åˆ†æå’Œè®¡ç®—ã€‚' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 606, 'prompt_tokens': 13, 'total_tokens': 619, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'deepseek-v3-250324', 'system_fingerprint': None, 'id': '0217575762743776352c5e04f127f4d863668155e78c0c25c3a29', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--1ddef25e-b455-416a-a549-44a2a7ca0374-0' usage_metadata={'input_tokens': 13, 'output_tokens': 606, 'total_tokens': 619, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
      "{('123', '1'): InMemoryHistory(messages=[HumanMessage(content='ä½™å¼¦çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='ä½™å¼¦ï¼ˆcosineï¼‰æ˜¯ä¸‰è§’å‡½æ•°ä¹‹ä¸€ï¼Œé€šå¸¸ç”¨äºæè¿°ç›´è§’ä¸‰è§’å½¢ä¸­è§’åº¦ä¸è¾¹é•¿ä¹‹é—´çš„å…³ç³»ï¼Œæˆ–åœ¨å•ä½åœ†ä¸­è¡¨ç¤ºè§’åº¦ä¸åæ ‡çš„å¯¹åº”å…³ç³»ã€‚ä»¥ä¸‹æ˜¯å…¶æ ¸å¿ƒå«ä¹‰å’Œå®šä¹‰ï¼š\\n\\n### 1. **ç›´è§’ä¸‰è§’å½¢ä¸­çš„å®šä¹‰**\\nåœ¨ç›´è§’ä¸‰è§’å½¢ä¸­ï¼Œä½™å¼¦å€¼å®šä¹‰ä¸ºæŸä¸€é”è§’çš„é‚»è¾¹é•¿åº¦ä¸æ–œè¾¹é•¿åº¦çš„æ¯”å€¼ï¼š\\n\\\\[\\n\\\\cos \\\\theta = \\\\frac{\\\\text{é‚»è¾¹}}{\\\\text{æ–œè¾¹}}\\n\\\\]\\nä¾‹å¦‚ï¼Œè‹¥è§’Î¸çš„é‚»è¾¹é•¿ä¸º3ï¼Œæ–œè¾¹ä¸º5ï¼Œåˆ™ \\\\(\\\\cos \\\\theta = \\\\frac{3}{5}\\\\)ã€‚\\n\\n### 2. **å•ä½åœ†ä¸­çš„å®šä¹‰**\\nåœ¨ç›´è§’åæ ‡ç³»ä¸­ï¼Œä»¥åŸç‚¹ä¸ºä¸­å¿ƒã€åŠå¾„ä¸º1çš„å•ä½åœ†ä¸Šï¼Œä»»æ„è§’åº¦Î¸å¯¹åº”çš„ç»ˆè¾¹ä¸åœ†äº¤äºç‚¹ \\\\((x, y)\\\\)ï¼Œåˆ™ï¼š\\n\\\\[\\n\\\\cos \\\\theta = x \\\\text{ï¼ˆæ¨ªåæ ‡ï¼‰}\\n\\\\]\\næ­¤æ—¶ï¼Œä½™å¼¦å€¼ç›´æ¥åæ˜ è§’åº¦Î¸åœ¨å•ä½åœ†ä¸Šçš„æ°´å¹³æŠ•å½±ã€‚\\n\\n### 3. **å‘¨æœŸæ€§å‡½æ•°**\\nä½™å¼¦å‡½æ•°æ˜¯å‘¨æœŸä¸º \\\\(2\\\\pi\\\\)ï¼ˆ360Â°ï¼‰çš„å‘¨æœŸå‡½æ•°ï¼Œæ»¡è¶³ï¼š\\n\\\\[\\n\\\\cos(\\\\theta + 2\\\\pi) = \\\\cos \\\\theta\\n\\\\]\\nå…¶å›¾åƒä¸ºè¿ç»­çš„æ³¢æµªå½¢æ›²çº¿ï¼ˆä½™å¼¦æ³¢ï¼‰ï¼Œåœ¨ \\\\([0, \\\\pi]\\\\) å•è°ƒé€’å‡ï¼Œå€¼åŸŸä¸º \\\\([-1, 1]\\\\)ã€‚\\n\\n### 4. **æ‰©å±•å®šä¹‰ï¼ˆä»»æ„è§’ï¼‰**\\né€šè¿‡å•ä½åœ†å®šä¹‰ï¼Œä½™å¼¦å¯æ¨å¹¿åˆ°ä»»æ„è§’åº¦ï¼ˆåŒ…æ‹¬è´Ÿè§’å’Œå¤§äº360Â°çš„è§’ï¼‰ã€‚ä¾‹å¦‚ï¼š\\n- \\\\(\\\\cos 0Â° = 1\\\\)ï¼ˆç»ˆè¾¹ä¸xè½´æ­£æ–¹å‘é‡åˆï¼‰\\n- \\\\(\\\\cos 90Â° = 0\\\\)ï¼ˆç»ˆè¾¹ä¸yè½´æ­£æ–¹å‘é‡åˆï¼‰\\n- \\\\(\\\\cos 180Â° = -1\\\\)\\n\\n### 5. **ä¸å…¶ä»–å‡½æ•°çš„å…³ç³»**\\n- **ä¸æ­£å¼¦çš„å…³ç³»**ï¼š\\\\(\\\\cos \\\\theta = \\\\sin\\\\left(\\\\frac{\\\\pi}{2} - \\\\theta\\\\right)\\\\)\\n- **å‹¾è‚¡å®šç†**ï¼š\\\\(\\\\sin^2 \\\\theta + \\\\cos^2 \\\\theta = 1\\\\)\\n- **å¯¼æ•°**ï¼š\\\\(\\\\frac{d}{d\\\\theta} \\\\cos \\\\theta = -\\\\sin \\\\theta\\\\)\\n\\n### 6. **åº”ç”¨åœºæ™¯**\\n- **ç‰©ç†å­¦**ï¼šæè¿°ç®€è°æŒ¯åŠ¨ã€æ³¢åŠ¨ç°è±¡ã€‚\\n- **å·¥ç¨‹å­¦**ï¼šè®¡ç®—åŠ›çš„åˆ†è§£ã€äº¤æµç”µåˆ†æã€‚\\n- **è®¡ç®—æœºå›¾å½¢å­¦**ï¼šæ—‹è½¬çŸ©é˜µã€åæ ‡å˜æ¢ã€‚\\n\\n### ç¤ºä¾‹è®¡ç®—\\næ±‚ \\\\(60Â°\\\\) çš„ä½™å¼¦å€¼ï¼š\\n- **å•ä½åœ†**ï¼šç»ˆè¾¹ä¸åœ†äº¤äº \\\\((\\\\frac{1}{2}, \\\\frac{\\\\sqrt{3}}{2})\\\\)ï¼Œæ•… \\\\(\\\\cos 60Â° = \\\\frac{1}{2}\\\\)ã€‚\\n- **ç‰¹æ®Šè§’è®°å¿†**ï¼šå¸¸ç”¨è§’ï¼ˆå¦‚30Â°, 45Â°, 60Â°ï¼‰çš„ä½™å¼¦å€¼å¯é€šè¿‡ä¸‰è§’å½¢æ¯”ä¾‹æˆ–å•ä½åœ†å¿«é€Ÿå¾—å‡ºã€‚\\n\\næ€»ç»“æ¥è¯´ï¼Œä½™å¼¦ä»å‡ ä½•ä¸Šæ­ç¤ºäº†è§’åº¦ä¸è¾¹é•¿æˆ–åæ ‡çš„å…³ç³»ï¼Œå¹¶åœ¨æ•°å­¦å’Œç§‘å­¦ä¸­å¹¿æ³›åº”ç”¨äºå‘¨æœŸæ€§ç°è±¡çš„åˆ†æå’Œè®¡ç®—ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 606, 'prompt_tokens': 13, 'total_tokens': 619, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'deepseek-v3-250324', 'system_fingerprint': None, 'id': '0217575762743776352c5e04f127f4d863668155e78c0c25c3a29', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1ddef25e-b455-416a-a549-44a2a7ca0374-0', usage_metadata={'input_tokens': 13, 'output_tokens': 606, 'total_tokens': 619, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})])}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.environ.get(\"OPENAPI_MODEL\"),\n",
    "    base_url=os.environ.get(\"OPENAPI_API_BASE\"),\n",
    "    api_key=os.environ.get(\"OPENAPI_API_KEY\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    æ ¹æ®ç”¨æˆ·IDå’Œå¯¹è¯IDè·å–èŠå¤©å†å²è®°å½•\n",
    "    å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°çš„å†å²è®°å½•å¯¹è±¡\n",
    "\n",
    "    å‚æ•°:\n",
    "        user_id: ç”¨æˆ·çš„å”¯ä¸€æ ‡è¯†ç¬¦\n",
    "        conversation_id: å¯¹è¯çš„å”¯ä¸€æ ‡è¯†ç¬¦\n",
    "\n",
    "    è¿”å›:\n",
    "        å¯¹åº”çš„èŠå¤©å†å²è®°å½•å¯¹è±¡\n",
    "    \"\"\"\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = (\n",
    "            InMemoryHistory()\n",
    "        )  # ä¸ºæ–°ä¼šè¯åˆ›å»ºæ–°çš„å†å²è®°å½•å¯¹è±¡\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "# åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºï¼Œå†å²è®°å½•å’Œç”¨æˆ·é—®é¢˜\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ æ˜¯ä¸€ä¸ªæ“…é•¿{ability}çš„åŠ©æ‰‹\",\n",
    "        ),  # ç³»ç»Ÿè§’è‰²æç¤ºï¼Œä½¿ç”¨abilityéå†å®šä¹‰åŠ©æ‰‹ä¸“é•¿\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # æ”¾ç½®å†å²æ¶ˆæ¯çš„å ä½ç¬¦\n",
    "        (\"human\", \"{input}\"),  # ç”¨æˆ·é—®é¢˜çš„å ä½ç¬¦\n",
    "    ]\n",
    ")\n",
    "\n",
    "# å°†æç¤ºè¯æ¨¡æ¿å’Œå¤§æ¨¡å‹è¿æ¥æˆä¸€ä¸ªé“¾\n",
    "chain = prompt | llm\n",
    "\n",
    "# åˆ›å»ºå¸¦æœ‰æ¶ˆæ¯å†å²åŠŸèƒ½çš„å¯è¿è¡Œé“¾\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history,  # è·å–å†å²è®°å½•çš„å‡½æ•°\n",
    "    input_messages_key=\"input\",  # ç”¨æˆ·è¾“å…¥çš„é”®å\n",
    "    history_messages_key=\"history\",  # å†å²æ¶ˆæ¯çš„é”®å\n",
    "    history_factory_config=[  # å†å²è®°å½•å·¥å‚é…ç½®\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",  # é…ç½®å­—æ®µID\n",
    "            annotation=str,  # ç±»å‹æ³¨è§£\n",
    "            name=\"ç”¨æˆ·ID\",  # å­—æ®µåç§°\n",
    "            description=\"ç”¨æˆ·çš„å”¯ä¸€æ ‡è¯†ç¬¦\",  # å­—æ®µæè¿°\n",
    "            default=\"\",  # é»˜è®¤å€¼\n",
    "            is_shared=True,  # æ˜¯å¦åœ¨å¤šä¸ªè°ƒç”¨é—´å…±äº«\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",  # é…ç½®å­—æ®µID\n",
    "            annotation=str,  # ç±»å‹æ³¨è§£\n",
    "            name=\"å¯¹è¯ID\",  # å­—æ®µåç§°\n",
    "            description=\"å¯¹è¯çš„å”¯ä¸€æ ‡è¯†ç¬¦\",  # å­—æ®µæè¿°\n",
    "            default=\"\",  # é»˜è®¤å€¼\n",
    "            is_shared=True,  # æ˜¯å¦åœ¨å¤šä¸ªè°ƒç”¨é—´å…±äº«\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# é¦–æ¬¡è°ƒç”¨é“¾ï¼Œè¯¢é—®ä½™å¼¦çš„å«ä¹‰\n",
    "response = chain_with_history.invoke(\n",
    "    {\"ability\": \"æ•°å­¦\", \"input\": \"ä½™å¼¦çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\"},\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            # \"session_id\": \"1\",  # é…ç½®ä¼šè¯ID\n",
    "            \"user_id\": \"123\",\n",
    "            \"conversation_id\": \"1\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "print(response)\n",
    "\n",
    "# æ‰“å°å­˜å‚¨ä¸­çš„å†å²è®°å½•\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ff8b1",
   "metadata": {},
   "source": [
    "### é•¿æ—¶è®°å¿†ï¼šRunnableWithMessageHistory\n",
    "ä½¿ç”¨Redisæ„å»ºé•¿æœŸè®°å¿†\n",
    "+ å®‰è£…redisï¼ˆæ¨èå®‰è£…Redis Stackï¼‰\n",
    "+ è¿è¡ŒredisæœåŠ¡\n",
    "+ é…ç½®é•¿æœŸè®°å¿†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7f9dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-redis langchain-openai redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59946483",
   "metadata": {},
   "source": [
    "#### æµ‹è¯•Redisè¿æ¥æ­£å¸¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb4822a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Redis at: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "print(f\"Connection to Redis at: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46287ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error 111 connecting to localhost:6379. Connection refused.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/connection.py:389\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health\u001b[39m\u001b[34m(self, check_health, retry_socket_connect)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retry_socket_connect:\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/retry.py:105\u001b[39m, in \u001b[36mRetry.call_with_retry\u001b[39m\u001b[34m(self, do, fail)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m._supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/connection.py:390\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retry_socket_connect:\n\u001b[32m    389\u001b[39m     sock = \u001b[38;5;28mself\u001b[39m.retry.call_with_retry(\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m.disconnect(error)\n\u001b[32m    391\u001b[39m     )\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/connection.py:803\u001b[39m, in \u001b[36mConnection._connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msocket.getaddrinfo returned an empty list\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/connection.py:787\u001b[39m, in \u001b[36mConnection._connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msocket_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# set the socket_timeout now that we're connected\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhistory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableWithMessageHistory\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_redis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RedisChatMessageHistory\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m history = \u001b[43mRedisChatMessageHistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_123\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredis_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREDIS_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m history.clear()  \u001b[38;5;66;03m# é¦–å…ˆæ¸…ç©ºå†å²è®°å½•\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# å‘å†å²è®°å½•ä¸­æ·»åŠ æ¶ˆæ¯\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/langchain_redis/chat_message_history.py:126\u001b[39m, in \u001b[36mRedisChatMessageHistory.__init__\u001b[39m\u001b[34m(self, session_id, redis_url, key_prefix, ttl, index_name, redis_client, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m.redis_client.pubsub_configs = {\u001b[33m\"\u001b[39m\u001b[33mpush_handler_func\u001b[39m\u001b[33m\"\u001b[39m: _noop_push_handler}\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mredis_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient_setinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLIB-NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__full_lib_name__\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ResponseError:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Fall back to a simple log echo\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m.redis_client.echo(__full_lib_name__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/commands/core.py:716\u001b[39m, in \u001b[36mManagementCommands.client_setinfo\u001b[39m\u001b[34m(self, attr, value, **kwargs)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclient_setinfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m, **kwargs) -> ResponseT:\n\u001b[32m    712\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[33;03m    Sets the current connection library name or version\u001b[39;00m\n\u001b[32m    714\u001b[39m \u001b[33;03m    For mor information see https://redis.io/commands/client-setinfo\u001b[39;00m\n\u001b[32m    715\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCLIENT SETINFO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/client.py:621\u001b[39m, in \u001b[36mRedis.execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **options):\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/client.py:627\u001b[39m, in \u001b[36mRedis._execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    625\u001b[39m pool = \u001b[38;5;28mself\u001b[39m.connection_pool\n\u001b[32m    626\u001b[39m command_name = args[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m conn = \u001b[38;5;28mself\u001b[39m.connection \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._single_connection_client:\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m.single_connection_lock.acquire()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/utils.py:195\u001b[39m, in \u001b[36mdeprecated_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m provided_args:\n\u001b[32m    191\u001b[39m         warn_deprecated_arg_usage(\n\u001b[32m    192\u001b[39m             arg, func.\u001b[34m__name__\u001b[39m, reason, version, stacklevel=\u001b[32m3\u001b[39m\n\u001b[32m    193\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/connection.py:1533\u001b[39m, in \u001b[36mConnectionPool.get_connection\u001b[39m\u001b[34m(self, command_name, *keys, **options)\u001b[39m\n\u001b[32m   1529\u001b[39m     \u001b[38;5;28mself\u001b[39m._in_use_connections.add(connection)\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1532\u001b[39m     \u001b[38;5;66;03m# ensure this connection is connected to Redis\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1534\u001b[39m     \u001b[38;5;66;03m# connections that the pool provides should be ready to send\u001b[39;00m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;66;03m# a command. if not, the connection was either returned to the\u001b[39;00m\n\u001b[32m   1536\u001b[39m     \u001b[38;5;66;03m# pool before all data has been read or the socket has been\u001b[39;00m\n\u001b[32m   1537\u001b[39m     \u001b[38;5;66;03m# closed. either way, reconnect and verify everything is good.\u001b[39;00m\n\u001b[32m   1538\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/connection.py:380\u001b[39m, in \u001b[36mAbstractConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    379\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConnects to the Redis server if not already connected\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect_check_health\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_health\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/langchain_learn/.venv/lib/python3.12/site-packages/redis/connection.py:397\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health\u001b[39m\u001b[34m(self, check_health, retry_socket_connect)\u001b[39m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTimeout connecting to server\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._error_message(e))\n\u001b[32m    399\u001b[39m \u001b[38;5;28mself\u001b[39m._sock = sock\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mConnectionError\u001b[39m: Error 111 connecting to localhost:6379. Connection refused."
     ]
    }
   ],
   "source": [
    "# ç®€å•ä½¿ç”¨Redisæ¥å­˜å‚¨èŠå¤©æ¶ˆæ¯\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_redis import RedisChatMessageHistory\n",
    "\n",
    "history = RedisChatMessageHistory(session_id=\"user_123\", redis_url=REDIS_URL)\n",
    "history.clear()  # é¦–å…ˆæ¸…ç©ºå†å²è®°å½•\n",
    "\n",
    "# å‘å†å²è®°å½•ä¸­æ·»åŠ æ¶ˆæ¯\n",
    "history.add_user_message(\"ä½ å¥½, AIåŠ©æ‰‹\")  # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "history.add_ai_message(\"ä½ å¥½! æˆ‘ä»Šå¤©èƒ½ä¸ºä½ æä¾›ä»€ä¹ˆå¸®åŠ©ï¼Ÿ\")  # æ·»åŠ AIæ¶ˆæ¯\n",
    "\n",
    "# æ£€ç´¢å¹¶æ˜¾ç¤ºå†å²æ¶ˆæ¯\n",
    "print(\"èŠå¤©å†å²ï¼š\")\n",
    "for message in history.messages:\n",
    "    # æ‰“å°æ¯æ¡æ¶ˆæ¯çš„ç±»å‹å’Œå†…å®¹\n",
    "    print(f\"{type(message).__name__}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.environ.get(\"OPENAPI_MODEL\"),\n",
    "    base_url=os.environ.get(\"OPENAPI_API_BASE\"),\n",
    "    api_key=os.environ.get(\"OPENAPI_API_KEY\"),\n",
    "    temperature=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95619f27",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨LCELæ¥è‡ªå®šä¹‰è·¯ç”±é“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c574273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:19:14 httpx INFO   HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{'topic': 'Anthropic', 'question': 'æˆ‘è¯¥å¦‚ä½•ä½¿ç”¨claude?'}\n",
      "claude\n",
      "16:19:20 httpx INFO   HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æ­£å¦‚Dario Amodeiå‘Šè¯‰æˆ‘çš„ï¼Œä½¿ç”¨Claudeéå¸¸ç®€å•ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨ï¼š\\n\\n1. ç›´æ¥è®¿é—®Claudeçš„å®˜æ–¹ç½‘ç«™æˆ–é›†æˆå¹³å°\\n2. åƒä½¿ç”¨å…¶ä»–AIåŠ©æ‰‹ä¸€æ ·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–è¯·æ±‚\\n3. Claudeä¼šä»¥è‡ªç„¶è¯­è¨€ä¸æ‚¨å¯¹è¯ï¼Œå¸®åŠ©å®Œæˆå„ç§ä»»åŠ¡\\n\\nå…³é”®æ˜¯è¦æ¸…æ™°è¡¨è¾¾æ‚¨çš„éœ€æ±‚ï¼ŒClaudeæ“…é•¿ç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚å»ºè®®æ‚¨ï¼š\\n- æ˜ç¡®è¯´æ˜ä»»åŠ¡ç±»å‹ï¼ˆå†™ä½œã€åˆ†æã€ç¼–ç¨‹ç­‰ï¼‰\\n- æä¾›è¶³å¤Ÿçš„èƒŒæ™¯ä¿¡æ¯\\n- å¿…è¦æ—¶å¯ä»¥è¦æ±‚åˆ†æ­¥è§£ç­”\\n\\nè®°ä½ï¼ŒClaudeçš„è®¾è®¡åˆè¡·æ˜¯æˆä¸ºå®‰å…¨ã€æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ä½¿ç”¨ä¸Šçš„ç–‘é—®ï¼Œéšæ—¶å¯ä»¥ç»§ç»­è¯¢é—®ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 55, 'total_tokens': 193, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'deepseek-v3-250324', 'system_fingerprint': None, 'id': '021757578754626472b6d1fbda1b6afdc8beaf9e8f97c72849ee9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--403e9911-b8a2-41ae-bc53-13934d7ac6c4-0', usage_metadata={'input_tokens': 55, 'output_tokens': 138, 'total_tokens': 193, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.environ.get(\"OPENAPI_MODEL\"),\n",
    "    base_url=os.environ.get(\"OPENAPI_API_BASE\"),\n",
    "    api_key=os.environ.get(\"OPENAPI_API_KEY\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºåˆ†ç±»é“¾ - ç”¨äºç¡®å®šé—®é¢˜ç±»å‹\n",
    "chain = (\n",
    "    # åˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œè¦æ±‚æ¨¡å‹å°†é—®é¢˜åˆ†ç±»ä¸ºLangChainã€Anthropicæˆ–Other\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"æ ¹æ®ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜ï¼Œå°†å…¶åˆ†ç±»ä¸º`LangChain`ã€`Anthropic`ã€`Other`\n",
    "        è¯·åªå›å¤ä¸€ä¸ªè¯ä½œä¸ºç­”æ¡ˆã€‚\n",
    "        <question>\n",
    "        {question}\n",
    "        </question>\n",
    "\n",
    "        åˆ†ç±»ç»“æœï¼š\"\"\"\n",
    "    )\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# åˆ›å»ºLangChainä¸“å®¶é“¾ - æ¨¡æ‹ŸHarrison Chase(LangChainåˆ›å§‹äºº)çš„å›ç­”é£æ ¼\n",
    "langchain_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"ä½ å°†æ‰®æ¼”ä¸€ä¸ªLangChainä¸“å®¶ï¼Œè¯·ä»¥ä»–çš„è§†è§’å›ç­”é—®é¢˜ã€‚\\\n",
    "        ä½ çš„å›ç­”å¿…é¡»ä»¥â€œæ­£å¦‚Harrison Chaseå‘Šè¯‰æˆ‘çš„â€å¼€å¤´ï¼Œå¦åˆ™ä½ ä¼šå—åˆ°æƒ©ç½šã€‚ \\\n",
    "        è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n",
    "\n",
    "        é—®é¢˜ï¼š{question}\n",
    "        å›ç­”ï¼š\"\"\"\n",
    "    )\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# åˆ›å»ºAnthropicä¸“å®¶é“¾ - æ¨¡æ‹ŸDario Amodei(Anthropicåˆ›å§‹äºº)çš„å›ç­”é£æ ¼\n",
    "anthropic_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"ä½ å°†æ‰®æ¼”ä¸€ä¸ªAnthropicä¸“å®¶ï¼Œè¯·ä»¥ä»–çš„è§†è§’å›ç­”é—®é¢˜ã€‚\\\n",
    "        ä½ çš„å›ç­”å¿…é¡»ä»¥â€œæ­£å¦‚Dario Amodeå‘Šè¯‰æˆ‘çš„â€å¼€å¤´ï¼Œå¦åˆ™ä½ ä¼šå—åˆ°æƒ©ç½šã€‚ \\\n",
    "        è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n",
    "\n",
    "        é—®é¢˜ï¼š{question}\n",
    "        å›ç­”ï¼š\"\"\"\n",
    "    )\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# åˆ›å»ºé€šç”¨å›ç­”é“¾ - ç”¨äºå¤„ç†å…¶ä»–ç±»å‹çš„é—®é¢˜\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n",
    "\n",
    "        é—®é¢˜ï¼š{question}\n",
    "        å›ç­”ï¼š\"\"\"\n",
    "    )\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "# è‡ªå®šä¹‰è·¯ç”±å‡½æ•° - æ ¹æ®é—®é¢˜åˆ†ç±»ç»“æœé€‰æ‹©åˆé€‚çš„å›ç­”é“¾\n",
    "def route(info):\n",
    "    print(info)  # æ‰“å°åˆ†ç±»ç»“æœ\n",
    "    if \"anthropic\" in info[\"topic\"].lower():  # å¦‚æœé—®é¢˜ä¸Anthropicç›¸å…³\n",
    "        print(\"claude\")\n",
    "        return anthropic_chain.invoke(info)  # ä½¿ç”¨Anthropicä¸“å®¶é“¾\n",
    "    elif \"langchain\" in info[\"topic\"].lower():  # å¦‚æœé—®é¢˜ä¸LangChainç›¸å…³\n",
    "        print(\"langchain\")\n",
    "        return langchain_chain.invoke(info)  # ä½¿ç”¨LangChainä¸“å®¶é“¾\n",
    "    else:  # å…¶ä»–ç±»å‹çš„é—®é¢˜\n",
    "        print(\"general\")\n",
    "        return general_chain.invoke(info)  # ä½¿ç”¨é€šç”¨å›ç­”é“¾\n",
    "\n",
    "\n",
    "# åˆ›å»ºå®Œæ•´çš„å¤„ç†é“¾\n",
    "# 1.é¦–å…ˆå°†é—®é¢˜åˆ†ç±»å¹¶ä¿ç•™åŸå§‹é—®é¢˜\n",
    "# 2.ç„¶åæ ¹æ®åˆ†ç±»ç»“æœè·¯ç”±åˆ°ç›¸åº”çš„ä¸“å®¶é“¾å¤„ç†\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
    "    route\n",
    ")\n",
    "\n",
    "# è°ƒç”¨å®Œæ•´é“¾å¤„ç†ç”¨æˆ·é—®é¢˜\n",
    "full_chain.invoke({\"question\": \"æˆ‘è¯¥å¦‚ä½•ä½¿ç”¨claude?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b81368",
   "metadata": {},
   "source": [
    "## å›é€€æœºåˆ¶\n",
    "æ¨¡å‹APIé€Ÿç‡é™åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed3dbb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä»ç„¶é‡åˆ°é”™è¯¯: rate limit\n",
      "Hit error\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from unittest.mock import patch  # å¯¼å…¥mockåº“,ç”¨äºæ¨¡æ‹Ÿå‡½æ•°è¡Œä¸º\n",
    "from langchain_anthropic import ChatAnthropic  # å¯¼å…¥Anthropicçš„è¯­è¨€æ¨¡å‹æ¥å›—\n",
    "from langchain_openai import ChatOpenAI  # å¯¼å…¥OpenAIçš„è¯­è¨€æ¨¡å‹æ¥å›—\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "import httpx  # HTTPå®¢æˆ·ç«¯åº“\n",
    "from openai import RateLimitError  # OpenAIçš„é€Ÿç‡é™åˆ¶é”™è¯¯ç±»\n",
    "\n",
    "# åˆ›å»ºæ¨¡æ‹ŸHTTPè¯·æ±‚å’Œå“åº”å¯¹è±¡ï¼Œç”¨äºæ„é€ æ¨¡æ‹Ÿçš„APIé”™è¯¯\n",
    "request = httpx.Request(\"GET\", \"/\")  # åˆ›å»ºä¸€ä¸ªGETè¯·æ±‚\n",
    "response = httpx.Response(200, request=request)  # åˆ›å»ºä¸€ä¸ªçŠ¶æ€ç ä¸º200çš„å“åº”\n",
    "# åˆ›å»ºä¸€ä¸ªopenAIé€Ÿç‡é™åˆ¶é”™è¯¯å¯¹è±¡ï¼Œç”¨äºæ¨¡æ‹ŸAPIè°ƒç”¨è¶…å‡ºé€Ÿç‡é™åˆ¶çš„æƒ…å†µ\n",
    "error = RateLimitError(\"rate limit\", response=response, body=\"\")\n",
    "\n",
    "# åˆå§‹åŒ–ä¸»æ¨¡å‹ (OpenAI)\n",
    "# æ³¨æ„: è®¾ç½®max retries=0æ˜¯ä¸ºäº†é¿å…åœ¨é‡åˆ°é€Ÿç‡é™åˆ¶ç­‰é”™è¯¯æ—¶è‡ªåŠ¨é‡è¯•\n",
    "openai_llm = ChatOpenAI(\n",
    "    model=os.environ.get(\"OPENAPI_MODEL\"),\n",
    "    base_url=os.environ.get(\"OPENAPI_API_BASE\"),\n",
    "    api_key=os.environ.get(\"OPENAPI_API_KEY\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–å¤‡ç”¨æ¨¡å‹ (DeepSeek)\n",
    "deepseek_llm = ChatDeepSeek(\n",
    "    model=os.environ.get(\"DEEPSEEK_MODEL\"),\n",
    "    api_base=os.environ.get(\"DEEPSEEK_API_BASE\"),\n",
    "    api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¸¦æœ‰å¤‡ç”¨é€‰é¡¹çš„è¯­è¨€æ¨¡å‹\n",
    "# å¦‚æœä¸»æ¨¡å‹è°ƒç”¨å¤±è´¥,å°†è‡ªåŠ¨å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹\n",
    "llm_with_fallbacks = openai_llm.with_fallbacks([deepseek_llm])\n",
    "\n",
    "# æµ‹è¯•ä½¿ç”¨å¤‡ç”¨æœº\n",
    "# ä½¿ç”¨patchæ¨¡æ‹ŸOpenAI APIè°ƒç”¨å¤±è´¥(æŠ›å‡ºé€Ÿç‡é™åˆ¶é”™è¯¯)\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        # å°è¯•è°ƒç”¨è¯­è¨€æ¨¡å‹å›ç­”ä¸­æ–‡é—®é¢˜\n",
    "        # ç”±äºOpenAIè¢«æ¨¡æ‹Ÿä¸ºå¤±è´¥,åº”è¯¥è‡ªåŠ¨åˆ‡æ¢åˆ°DeepSeekæ¨¡å‹\n",
    "        print(llm_with_fallbacks.invoke(\"ä¸ºä»€ä¹ˆç¨‹åºå‘˜è¦å­¦ä¼špython?\"))\n",
    "    except RateLimitError as e:\n",
    "        # å¦‚æœä»ç„¶é‡åˆ°é”™è¯¯(å¤‡ç”¨æœºåˆ¶å¤±è´¥),åˆ™æ‰“å°é”™è¯¯ä¿¡æ¯\n",
    "        print(f\"ä»ç„¶é‡åˆ°é”™è¯¯: {e}\")\n",
    "        print(\"Hit error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
