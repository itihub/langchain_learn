{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from openai.env file\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Read the OPENAI_API_KEY from the environment\n",
    "api_base = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = api_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatDoc:实现一个智能文档助手\n",
    "\n",
    "功能：\n",
    "- 读取pdf、excel、doc三种常见的文档格式\n",
    "- 根据文档内容，智能抽取内容并输出相应格式\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 安装相关的依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (pyproject.toml): started\n",
      "  Building wheel for docx2txt (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=4003 sha256=a18fb258065a2bae31dace128fc004309c101ddc0741ad17d394732c7c8b26ce\n",
      "  Stored in directory: c:\\users\\jizhe\\appdata\\local\\pip\\cache\\wheels\\6f\\f7\\05\\c745e7756faa8641660b6159b979deb122f2e3e1e0d9287eeb\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pypdf in c:\\users\\jizhe\\.virtualenvs\\lang-chain-wyhjbg1w\\lib\\site-packages (5.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\jizhe\\.virtualenvs\\lang-chain-wyhjbg1w\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\jizhe\\.virtualenvs\\lang-chain-wyhjbg1w\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\jizhe\\.virtualenvs\\lang-chain-wyhjbg1w\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jizhe\\.virtualenvs\\lang-chain-wyhjbg1w\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jizhe\\.virtualenvs\\lang-chain-wyhjbg1w\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jizhe\\.virtualenvs\\lang-chain-wyhjbg1w\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install docx2txt\n",
    "%pip install pypdf\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 测试加载不同格式的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './sources/fake.docx'}, page_content='一、公司基本信息\\n\\n名称：宏图科技发展有限公司\\n\\n注册地址：江苏省南京市雨花台区软件大道101号\\n\\n成立日期：2011年5月16日\\n\\n法定代表人：李强\\n\\n注册资本：人民币5000万元\\n\\n员工人数：约200人\\n\\n联系电话：025-88888888\\n\\n电子邮箱：info@hongtutech.cn\\n\\n\\n\\n二、财务状况概述\\n\\n截至2023年第一季度，宏图科技发展有限公司财务状况堪忧，具体情况如下：\\n\\n1. 资产总额：人民币1.2亿元，较上年同期下降30%。\\n\\n2. 负债总额：人民币1.8亿元，较上年同期上升50%，资不抵债。\\n\\n3. 营业收入：人民币3000万元，较上年同期下降60%。\\n\\n4. 净利润：亏损人民币800万元，去年同期为盈利人民币200万元。\\n\\n5. 现金流量：公司现金流量紧张，现金及现金等价物余额为人民币500万元，难以支撑日常运营。\\n\\n6. 存货：存货积压严重，库存商品价值约为人民币400万元，大部分产品滞销。\\n\\n7. 应收账款：应收账款高达人民币600万元，回收难度大，坏账准备不足。\\n\\n\\n\\n三、主营业务及市场状况\\n\\n宏图科技发展有限公司主要从事计算机软件的研发与销售。近年来，由于市场竞争加剧、技术更新换代速度快和管理层决策失误等原因，公司主营业务收入持续下降。目前，公司面临的主要问题有：\\n\\n1. 产品同质化严重，缺乏核心竞争力。\\n\\n2. 新产品开发进度缓慢，未能及时抓住市场需求变化。\\n\\n3. 市场营销策略不当，导致市场份额大幅缩水。\\n\\n4. 行业内新兴企业崛起迅速，原有客户流失严重。\\n\\n\\n\\n四、债权债务情况\\n\\n宏图科技发展有限公司目前面临的债务问题严峻，具体情况如下：\\n\\n1. 银行贷款：公司向多家银行贷款总额达人民币1亿元，部分贷款已逾期未还。\\n\\n2. 供应商欠款：因现金流紧张，公司拖欠供应商货款达人民币300万元。\\n\\n3. 员工工资及社保：由于资金链断裂，公司拖欠员工工资及社保费用共计人民币200万元。\\n\\n4. 其他应付款项：包括税费、租赁费用等其他应付款项累计约人民币100万元。\\n\\n\\n\\n五、资产清单\\n\\n宏图科技发展有限公司目前拥有的主要资产包括：\\n\\n1. 固定资产：公司办公用房和设备原值合计人民币800万元，累计折旧约400万元。\\n\\n2. 无形资产：包括软件著作权、专利权等无形资产原值合计人民币300万元。\\n\\n3. 存货资产：存货包括已完成软件产品和半成品，价值约为人民币400万元。\\n\\n4. 应收账款：主要包括对外销售软件的应收账款合计人民币600万元。\\n\\n\\n\\n六、潜在风险及预警\\n\\n1. 经营风险：由于连续亏损，公司可能面临破产清算的风险。\\n\\n2. 债务风险：负债累累，若短期内无法筹措足够资金偿还债务，可能面临诉讼或资产被查封的风险。\\n\\n3. 市场风险：行业竞争加剧和市场需求不明朗，可能导致公司未来业绩继续恶化。\\n\\n4. 法律风险：因未能按时支付债务和相关费用，可能面临相关法律诉讼或处罚。\\n\\n\\n\\n七、结论与建议\\n\\n综上所述，宏图科技发展有限公司目前处于财务困境之中，若无外部资金注入或业务转型成功，短期内难以扭转局势。对于不良资产收购方来说，在考虑收购宏图科技的相关资产前，建议进行深入的尽职调查，并制定详细的风险控制和资产处置方案。同时，在估值时应充分考虑到公司所面临的各种潜在风险和清收难度。\\n\\n\\n\\n报告撰写日期：2023年4月20日')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载docx文件\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "# 定义方法\n",
    "def getFile():\n",
    "    # 读取docx文件\n",
    "    loader = Docx2txtLoader(\"./sources/fake.docx\")\n",
    "    text = loader.load()\n",
    "    return text;\n",
    "\n",
    "# 调用方法\n",
    "getFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS 版本14.1.1（版号23B81） Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20231205083748Z00'00'\", 'moddate': \"D:20231205083748Z00'00'\", 'source': './sources/fake.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='一、公司基本信息 名称：宏图科技发展有限公司 注册地址：江苏省南京市雨花台区软件大道101号 成立日期：2011年5月16日 法定代表人：李强 注册资本：人民币5000万元 员工人数：约200人 联系电话：025-88888888 电子邮箱：info@hongtutech.cn  二、财务状况概述 截至2023年第一季度，宏图科技发展有限公司财务状况堪忧，具体情况如下： 1. 资产总额：人民币1.2亿元，较上年同期下降30%。 2. 负债总额：人民币1.8亿元，较上年同期上升50%，资不抵债。 3. 营业收入：人民币3000万元，较上年同期下降60%。 4. 净利润：亏损人民币800万元，去年同期为盈利人民币200万元。 5. 现金流量：公司现金流量紧张，现金及现金等价物余额为人民币500万元，难以支撑日常运营。 6. 存货： 存货积压严重， 库存商品价值约为人民币400万元， 大部分产品滞销。 7. 应收账款：应收账款高达人民币600万元，回收难度大，坏账准备不足。  三、主营业务及市场状况 宏图科技发展有限公司主要从事计算机软件的研发与销售。近年来，由于市场竞争加剧、技术更新换代速度快和管理层决策失误等原因，公司主营业务收入持续下降。目前，公司面临的主要问题有： 1. 产品同质化严重，缺乏核心竞争力。 2. 新产品开发进度缓慢，未能及时抓住市场需求变化。 3. 市场营销策略不当，导致市场份额大幅缩水。'),\n",
       " Document(metadata={'producer': 'macOS 版本14.1.1（版号23B81） Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20231205083748Z00'00'\", 'moddate': \"D:20231205083748Z00'00'\", 'source': './sources/fake.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='4. 行业内新兴企业崛起迅速，原有客户流失严重。  四、债权债务情况 宏图科技发展有限公司目前面临的债务问题严峻，具体情况如下： 1. 银行贷款： 公司向多家银行贷款总额达人民币1亿元， 部分贷款已逾期未还。 2. 供应商欠款：因现金流紧张，公司拖欠供应商货款达人民币300万元。 3. 员工工资及社保：由于资金链断裂，公司拖欠员工工资及社保费用共计人民币200万元。 4. 其他应付款项： 包括税费、 租赁费用等其他应付款项累计约人民币100万元。  五、资产清单 宏图科技发展有限公司目前拥有的主要资产包括： 1. 固定资产：公司办公用房和设备原值合计人民币800万元，累计折旧约400万元。 2. 无形资产：包括软件著作权、专利权等无形资产原值合计人民币300万元。 3. 存货资产：存货包括已完成软件产品和半成品，价值约为人民币400万元。 4. 应收账款：主要包括对外销售软件的应收账款合计人民币600万元。  六、潜在风险及预警 1. 经营风险：由于连续亏损，公司可能面临破产清算的风险。 2. 债务风险：负债累累，若短期内无法筹措足够资金偿还债务，可能面临诉讼或资产被查封的风险。 3. 市场风险：行业竞争加剧和市场需求不明朗，可能导致公司未来业绩继续恶化。 4. 法律风险： 因未能按时支付债务和相关费用， 可能面临相关法律诉讼或处罚。  七、结论与建议 综上所述，宏图科技发展有限公司目前处于财务困境之中，若无外部资金注入或业务转型成功，短期内难以扭转局势。对于不良资产收购方来说，在考虑收购宏'),\n",
       " Document(metadata={'producer': 'macOS 版本14.1.1（版号23B81） Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20231205083748Z00'00'\", 'moddate': \"D:20231205083748Z00'00'\", 'source': './sources/fake.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='图科技的相关资产前，建议进行深入的尽职调查，并制定详细的风险控制和资产处置方案。 同时， 在估值时应充分考虑到公司所面临的各种潜在风险和清收难度。  报告撰写日期：2023年4月20日')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载pdf文件\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 定义方法\n",
    "def getFile():\n",
    "    try:\n",
    "        # 读取pdf文件\n",
    "        loader = PyPDFLoader(\"./sources/fake.pdf\")\n",
    "        text = loader.load()\n",
    "        return text;\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files:{e}\")\n",
    "\n",
    "# 调用方法\n",
    "getFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './sources/fake.xlsx', 'file_directory': './sources', 'filename': 'fake.xlsx', 'last_modified': '2024-04-12T12:05:13', 'page_name': 'Sheet1', 'page_number': 1, 'text_as_html': '<table><tr><td>名称</td><td>宏图科技发展有限公司</td></tr><tr><td>注册地址</td><td>江苏省南京市雨花台区软件大道101号</td></tr><tr><td>成立日期</td><td>40679</td></tr><tr><td>法定代表人</td><td>李强</td></tr><tr><td>注册资本</td><td>人民币5000万元</td></tr><tr><td>员工人数</td><td>约200人</td></tr><tr><td>联系电话</td><td>025-88888888</td></tr><tr><td>电子邮箱</td><td>info@hongtutech.cn</td></tr><tr><td>资产总额</td><td>人民币1.2亿元，较上年同期下降30%</td></tr><tr><td>负债总额</td><td>人民币1.8亿元，较上年同期上升50%，资不抵债</td></tr><tr><td>营业收入</td><td>人民币3000万元，较上年同期下降60%</td></tr><tr><td>净利润</td><td>亏损人民币800万元，去年同期为盈利人民币200万元</td></tr><tr><td>现金流量</td><td>公司现金流量紧张，现金及现金等价物余额为人民币500万元，难以支撑日常运营</td></tr></table>', 'languages': ['zho', 'kor'], 'filetype': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'category': 'Table', 'element_id': '7ed30bdc7ad64071dae72ac7a379911a'}, page_content='名称 宏图科技发展有限公司 注册地址 江苏省南京市雨花台区软件大道101号 成立日期 40679 法定代表人 李强 注册资本 人民币5000万元 员工人数 约200人 联系电话 025-88888888 电子邮箱 info@hongtutech.cn 资产总额 人民币1.2亿元，较上年同期下降30% 负债总额 人民币1.8亿元，较上年同期上升50%，资不抵债 营业收入 人民币3000万元，较上年同期下降60% 净利润 亏损人民币800万元，去年同期为盈利人民币200万元 现金流量 公司现金流量紧张，现金及现金等价物余额为人民币500万元，难以支撑日常运营')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载excel文件\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "# 定义方法\n",
    "def getFile():\n",
    "    try:\n",
    "        # 读取xlsx文件\n",
    "        loader = UnstructuredExcelLoader(\"./sources/fake.xlsx\", mode=\"elements\")\n",
    "        text = loader.load()\n",
    "        return text;\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files:{e}\")\n",
    "\n",
    "# 调用方法\n",
    "getFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 整合1：动态文档加载器：支持多种格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './sources/fake.xlsx'}, page_content='名称 宏图科技发展有限公司 注册地址 江苏省南京市雨花台区软件大道101号 成立日期 40679 法定代表人 李强 注册资本 人民币5000万元 员工人数 约200人 联系电话 025-88888888 电子邮箱 info@hongtutech.cn 资产总额 人民币1.2亿元，较上年同期下降30% 负债总额 人民币1.8亿元，较上年同期上升50%，资不抵债 营业收入 人民币3000万元，较上年同期下降60% 净利润 亏损人民币800万元，去年同期为盈利人民币200万元 现金流量 公司现金流量紧张，现金及现金等价物余额为人民币500万元，难以支撑日常运营')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader, Docx2txtLoader, PyPDFLoader\n",
    "\n",
    "# 定义 ChatDoc 类\n",
    "class ChatDoc():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 定义类变量\n",
    "        self.doc = None # 用于存储文件路径\n",
    "\n",
    "    def getFile(self):\n",
    "        \"\"\"\n",
    "        使用 LangChain 的加载器，加载不同类型的文件（docx, pdf, xlsx）\n",
    "        \"\"\"\n",
    "        # 获取 self.doc 中存储的文件路径\n",
    "        doc = self.doc \n",
    "        # 创建一个字典，将文件扩展名映射到对应的 LangChain 加载器类。\n",
    "        loaders = {\n",
    "            \"docx\": Docx2txtLoader,\n",
    "            \"pdf\": PyPDFLoader,\n",
    "            \"xlsx\": UnstructuredExcelLoader,\n",
    "        }\n",
    "        # 从文件路径中提取文件扩展名\n",
    "        file_extension = doc.split(\".\")[-1]\n",
    "        # 根据文件扩展名从 loaders 字典中获取加载器类\n",
    "        loader_class = loaders.get(file_extension)\n",
    "        if loader_class:\n",
    "            try:\n",
    "                # 创建加载器实例\n",
    "                loader = loader_class(doc)\n",
    "                # 加载文件内容，返回 Document 对象列表\n",
    "                text = loader.load()\n",
    "                return text\n",
    "            except Exception as e: \n",
    "                print(f\"Error loading {file_extension} files:{e}\") \n",
    "        else:\n",
    "            # 如果没有找到对应的加载器类，打印异常信息\n",
    "            print(f\"Unsupported file extension: {file_extension}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# 创建 ChatDoc 实例并调用 getFile 方法\n",
    "chat_doc = ChatDoc()\n",
    "chat_doc.doc = \"./sources/fake.xlsx\"\n",
    "chat_doc.getFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.整合2：增加了文本切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './sources/fake.xlsx'}, page_content='名称 宏图科技发展有限公司 注册地址 江苏省南京市雨花台区软件大道101号 成立日期 40679 法定代表人 李强 注册资本 人民币5000万元 员工人数 约200人 联系电话 025-88888888 电子邮箱 info@hongtutech.cn 资产总额 人民币1.2亿元，较上年同期下降30% 负债总额 人民币1.8亿元，较上年同期上升50%，资不抵债 营业收入 人民币3000万元，较上年同期下降60% 净利润 亏损人民币800万元，去年同期为盈利人民币200万元 现金流量 公司现金流量紧张，现金及现金等价物余额为人民币500万元，难以支撑日常运营')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader, Docx2txtLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 定义 ChatDoc 类\n",
    "class ChatDoc():\n",
    "    \n",
    "    def __init__(self):\n",
    "         # 定义类变量\n",
    "        self.doc = None\n",
    "        self.splitText = [] # 用户存储分割后的文本\n",
    "\n",
    "    def getFile(self):\n",
    "        doc = self.doc\n",
    "        loaders = {\n",
    "            \"docx\":Docx2txtLoader,\n",
    "            \"pdf\":PyPDFLoader,\n",
    "            \"xlsx\":UnstructuredExcelLoader,\n",
    "        }\n",
    "        file_extension = doc.split(\".\")[-1]\n",
    "        loader_class = loaders.get(file_extension)\n",
    "        if loader_class:\n",
    "            try:\n",
    "                loader = loader_class(doc)\n",
    "                text = loader.load()\n",
    "                return text\n",
    "            except Exception as e: \n",
    "                print(f\"Error loading {file_extension} files:{e}\") \n",
    "        else:\n",
    "             print(f\"Unsupported file extension: {file_extension}\")\n",
    "             return  None \n",
    "\n",
    "    def splitSentences(self):\n",
    "        \"\"\"\n",
    "        处理文档分割的函数\n",
    "        \"\"\"\n",
    "        full_text = self.getFile() # 获取文档内容\n",
    "        if full_text != None:\n",
    "            # 对文档进行分割\n",
    "            text_split = CharacterTextSplitter(\n",
    "                chunk_size=150,\n",
    "                chunk_overlap=20,\n",
    "            )\n",
    "            texts = text_split.split_documents(full_text)\n",
    "            self.splitText = texts\n",
    "\n",
    "\n",
    "# 创建 ChatDoc 实例进行测试\n",
    "chat_doc = ChatDoc()\n",
    "chat_doc.doc = \"./sources/fake.xlsx\"\n",
    "chat_doc.splitSentences()\n",
    "print(chat_doc.splitText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.整合3：将分割后文本块进行向量化与索引存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x28f8c2c50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader, Docx2txtLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 定义 ChatDoc 类\n",
    "class ChatDoc():\n",
    "    \n",
    "    def __init__(self):\n",
    "         # 定义类变量\n",
    "        self.doc = None\n",
    "        self.splitText = [] # 用户存储分割后的文本\n",
    "\n",
    "    def getFile(self):\n",
    "        doc = self.doc\n",
    "        loaders = {\n",
    "            \"docx\":Docx2txtLoader,\n",
    "            \"pdf\":PyPDFLoader,\n",
    "            \"xlsx\":UnstructuredExcelLoader,\n",
    "        }\n",
    "        file_extension = doc.split(\".\")[-1]\n",
    "        loader_class = loaders.get(file_extension)\n",
    "        if loader_class:\n",
    "            try:\n",
    "                loader = loader_class(doc)\n",
    "                text = loader.load()\n",
    "                return text\n",
    "            except Exception as e: \n",
    "                print(f\"Error loading {file_extension} files:{e}\") \n",
    "        else:\n",
    "             print(f\"Unsupported file extension: {file_extension}\")\n",
    "             return  None \n",
    "\n",
    "    def splitSentences(self):\n",
    "        \"\"\"\n",
    "        处理文档分割的函数\n",
    "        \"\"\"\n",
    "        full_text = self.getFile() # 获取文档内容\n",
    "        if full_text != None:\n",
    "            # 对文档进行分割\n",
    "            text_split = CharacterTextSplitter(\n",
    "                chunk_size=150,\n",
    "                chunk_overlap=20,\n",
    "            )\n",
    "            texts = text_split.split_documents(full_text)\n",
    "            self.splitText = texts\n",
    "    \n",
    "    def embeddingAndVectorDB(self):\n",
    "        \"\"\"\n",
    "        将分割后的文本块进行向量化并存储到 Chroma 向量数据库中\n",
    "        \"\"\"\n",
    "        # 创建嵌入模型\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        # 创建一个 Chroma 向量数据库实例\n",
    "        db = Chroma.from_documents(\n",
    "            documents = self.splitText, # 分割后文档的列表\n",
    "            embedding = embeddings, # 嵌入模型实例\n",
    "        )\n",
    "        # 返回创建的 Chroma 向量数据库实例\n",
    "        return db\n",
    "\n",
    "# 创建 ChatDoc 实例进行测试\n",
    "chat_doc = ChatDoc()\n",
    "chat_doc.doc = \"./sources/fake.docx\"\n",
    "chat_doc.splitSentences()\n",
    "chat_doc.embeddingAndVectorDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.整合4：通过向量数据库使用自然语言找出相关的文本块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jizhe\\AppData\\Local\\Temp\\ipykernel_48020\\3877044334.py:53: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'cli...20, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     75\u001b[39m chat_doc.doc = \u001b[33m\"\u001b[39m\u001b[33m./sources/fake.docx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m chat_doc.splitSentences()\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mchat_doc\u001b[49m\u001b[43m.\u001b[49m\u001b[43maskAndFindFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m这家公司叫什么名字?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mChatDoc.askAndFindFiles\u001b[39m\u001b[34m(self, question)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m接受一个问题（question）作为输入，并在向量数据库中查找与该问题相关的文档\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# 创建向量数据库\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m db = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddingAndVectorDB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 创建检索器:将向量数据库实例 db 转换为一个检索器（retriever）对象。检索器用于根据查询语句在向量数据库中查找相关文档。\u001b[39;00m\n\u001b[32m     69\u001b[39m retriever = db.as_retriever()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mChatDoc.embeddingAndVectorDB\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m将分割后的文本块进行向量化并存储到 Chroma 向量数据库中\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 创建嵌入模型\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m embeddings = \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# 创建一个 Chroma 向量数据库实例\u001b[39;00m\n\u001b[32m     55\u001b[39m db = Chroma.from_documents(\n\u001b[32m     56\u001b[39m     documents = \u001b[38;5;28mself\u001b[39m.splitText, \u001b[38;5;66;03m# 分割后文档的列表\u001b[39;00m\n\u001b[32m     57\u001b[39m     embedding = embeddings, \u001b[38;5;66;03m# 嵌入模型实例\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jizhe\\.virtualenvs\\lang-chain-wYHJBg1W\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    213\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jizhe\\.virtualenvs\\lang-chain-wYHJBg1W\\Lib\\site-packages\\pydantic\\main.py:214\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    213\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    216\u001b[39m     warnings.warn(\n\u001b[32m    217\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    218\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    220\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for OpenAIEmbeddings\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'cli...20, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader, Docx2txtLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 定义 ChatDoc 类\n",
    "class ChatDoc():\n",
    "    \n",
    "    def __init__(self):\n",
    "         # 定义类变量\n",
    "        self.doc = None\n",
    "        self.splitText = [] # 用户存储分割后的文本\n",
    "\n",
    "    def getFile(self):\n",
    "        doc = self.doc\n",
    "        loaders = {\n",
    "            \"docx\":Docx2txtLoader,\n",
    "            \"pdf\":PyPDFLoader,\n",
    "            \"xlsx\":UnstructuredExcelLoader,\n",
    "        }\n",
    "        file_extension = doc.split(\".\")[-1]\n",
    "        loader_class = loaders.get(file_extension)\n",
    "        if loader_class:\n",
    "            try:\n",
    "                loader = loader_class(doc)\n",
    "                text = loader.load()\n",
    "                return text\n",
    "            except Exception as e: \n",
    "                print(f\"Error loading {file_extension} files:{e}\") \n",
    "        else:\n",
    "             print(f\"Unsupported file extension: {file_extension}\")\n",
    "             return  None \n",
    "\n",
    "    def splitSentences(self):\n",
    "        \"\"\"\n",
    "        处理文档分割的函数\n",
    "        \"\"\"\n",
    "        full_text = self.getFile() # 获取文档内容\n",
    "        if full_text != None:\n",
    "            # 对文档进行分割\n",
    "            text_split = CharacterTextSplitter(\n",
    "                chunk_size=150,\n",
    "                chunk_overlap=20,\n",
    "            )\n",
    "            texts = text_split.split_documents(full_text)\n",
    "            self.splitText = texts\n",
    "    \n",
    "    def embeddingAndVectorDB(self):\n",
    "        \"\"\"\n",
    "        将分割后的文本块进行向量化并存储到 Chroma 向量数据库中\n",
    "        \"\"\"\n",
    "        # 创建嵌入模型\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        # 创建一个 Chroma 向量数据库实例\n",
    "        db = Chroma.from_documents(\n",
    "            documents = self.splitText, # 分割后文档的列表\n",
    "            embedding = embeddings, # 嵌入模型实例\n",
    "        )\n",
    "        # 返回创建的 Chroma 向量数据库实例\n",
    "        return db\n",
    "    \n",
    "    def askAndFindFiles(self, question):\n",
    "        \"\"\"\n",
    "        接受一个问题（question）作为输入，并在向量数据库中查找与该问题相关的文档\n",
    "        \"\"\"\n",
    "        # 创建向量数据库\n",
    "        db = self.embeddingAndVectorDB()\n",
    "        # 创建检索器:将向量数据库实例 db 转换为一个检索器（retriever）对象。检索器用于根据查询语句在向量数据库中查找相关文档。\n",
    "        retriever = db.as_retriever()\n",
    "        # 执行检索\n",
    "        results = retriever.invoke(question)\n",
    "        return results\n",
    "\n",
    "# 创建 ChatDoc 实例进行测试\n",
    "chat_doc = ChatDoc()\n",
    "chat_doc.doc = \"./sources/fake.docx\"\n",
    "chat_doc.splitSentences()\n",
    "chat_doc.askAndFindFiles(\"这家公司叫什么名字?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.优化1：多重查询：向量数据库与LLM协同提升检索精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "/Users/tomiezhang/.pyenv/versions/3.10.12/envs/langchains/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the name of the company?', 'What is the official title of the company?', 'Can you tell me the name of the organization?']\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='一、公司基本信息\\n\\n名称：宏图科技发展有限公司\\n\\n注册地址：江苏省南京市雨花台区软件大道101号\\n\\n成立日期：2011年5月16日\\n\\n法定代表人：李强\\n\\n注册资本：人民币5000万元\\n\\n员工人数：约200人\\n\\n联系电话：025-88888888\\n\\n电子邮箱：info@hongtutech.cn', metadata={'source': 'example/fake.docx'}), Document(page_content='7. 应收账款：应收账款高达人民币600万元，回收难度大，坏账准备不足。\\n\\n三、主营业务及市场状况\\n\\n宏图科技发展有限公司主要从事计算机软件的研发与销售。近年来，由于市场竞争加剧、技术更新换代速度快和管理层决策失误等原因，公司主营业务收入持续下降。目前，公司面临的主要问题有：', metadata={'source': 'example/fake.docx'}), Document(page_content='4. 其他应付款项：包括税费、租赁费用等其他应付款项累计约人民币100万元。\\n\\n五、资产清单\\n\\n宏图科技发展有限公司目前拥有的主要资产包括：\\n\\n1. 固定资产：公司办公用房和设备原值合计人民币800万元，累计折旧约400万元。', metadata={'source': 'example/fake.docx'}), Document(page_content='报告撰写日期：2023年4月20日', metadata={'source': 'example/fake.docx'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader, Docx2txtLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# 引入openai和多重向量检索\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "\n",
    "# 定义 ChatDoc 类\n",
    "class ChatDoc():\n",
    "    \n",
    "    def __init__(self):\n",
    "         # 定义类变量\n",
    "        self.doc = None\n",
    "        self.splitText = [] # 用户存储分割后的文本\n",
    "\n",
    "    def getFile(self):\n",
    "        doc = self.doc\n",
    "        loaders = {\n",
    "            \"docx\":Docx2txtLoader,\n",
    "            \"pdf\":PyPDFLoader,\n",
    "            \"xlsx\":UnstructuredExcelLoader,\n",
    "        }\n",
    "        file_extension = doc.split(\".\")[-1]\n",
    "        loader_class = loaders.get(file_extension)\n",
    "        if loader_class:\n",
    "            try:\n",
    "                loader = loader_class(doc)\n",
    "                text = loader.load()\n",
    "                return text\n",
    "            except Exception as e: \n",
    "                print(f\"Error loading {file_extension} files:{e}\") \n",
    "        else:\n",
    "             print(f\"Unsupported file extension: {file_extension}\")\n",
    "             return  None \n",
    "\n",
    "    def splitSentences(self):\n",
    "        \"\"\"\n",
    "        处理文档分割的函数\n",
    "        \"\"\"\n",
    "        full_text = self.getFile() # 获取文档内容\n",
    "        if full_text != None:\n",
    "            # 对文档进行分割\n",
    "            text_split = CharacterTextSplitter(\n",
    "                chunk_size=150,\n",
    "                chunk_overlap=20,\n",
    "            )\n",
    "            texts = text_split.split_documents(full_text)\n",
    "            self.splitText = texts\n",
    "    \n",
    "    def embeddingAndVectorDB(self):\n",
    "        \"\"\"\n",
    "        将分割后的文本块进行向量化并存储到 Chroma 向量数据库中\n",
    "        \"\"\"\n",
    "        # 创建嵌入模型\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        # 创建一个 Chroma 向量数据库实例\n",
    "        db = Chroma.from_documents(\n",
    "            documents = self.splitText, # 分割后文档的列表\n",
    "            embedding = embeddings, # 嵌入模型实例\n",
    "        )\n",
    "        # 返回创建的 Chroma 向量数据库实例\n",
    "        return db\n",
    "    \n",
    "    def askAndFindFiles(self, question):\n",
    "        \"\"\"\n",
    "        接受一个问题（question）作为输入，并在向量数据库中查找与该问题相关的文档\n",
    "        MultiQueryRetriever 的作用是：\n",
    "        + 接受一个原始查询。\n",
    "        + 使用 LLM 生成多个与原始查询相关的变体查询。\n",
    "        + 使用这些变体查询在向量数据库中检索相关文档。\n",
    "        + 将检索结果合并并返回。\n",
    "        \"\"\"\n",
    "        # 创建向量数据库\n",
    "        db = self.embeddingAndVectorDB()\n",
    "        # 创建一个 ChatOpenAI 实例，用于生成多个相关查询。\n",
    "        # temperature=0 表示模型将始终返回最可能的答案，从而提高结果的确定性。\n",
    "        llm = ChatOpenAI(temperature=0)\n",
    "        retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "            retriever = db.as_retriever(),\n",
    "            llm = llm,\n",
    "        )\n",
    "        return retriever_from_llm.get_relevant_documents(question)\n",
    "    \n",
    "\n",
    "# 创建 ChatDoc 实例进行测试\n",
    "chat_doc = ChatDoc()\n",
    "chat_doc.doc = \"./sources/fake.docx\"\n",
    "chat_doc.splitSentences()\n",
    "\n",
    "# 设置下logging查看生成查询\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.DEBUG)\n",
    "unique_doc = chat_doc.askAndFindFiles(\"公司名称是什么?\")\n",
    "print(unique_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.优化2：使用上下文压缩检索降低冗余信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "/Users/tomiezhang/.pyenv/versions/3.10.12/envs/langchains/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "/Users/tomiezhang/.pyenv/versions/3.10.12/envs/langchains/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "/Users/tomiezhang/.pyenv/versions/3.10.12/envs/langchains/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "/Users/tomiezhang/.pyenv/versions/3.10.12/envs/langchains/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "/Users/tomiezhang/.pyenv/versions/3.10.12/envs/langchains/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='银行贷款：公司向多家银行贷款总额达人民币1亿元，部分贷款已逾期未还。 供应商欠款：因现金流紧张，公司拖欠供应商货款达人民币300万元。 员工工资及社保：由于资金链断裂，公司拖欠员工工资及社保费用共计人民币200万元。', metadata={'source': 'example/fake.docx'}), Document(page_content='银行贷款：公司向多家银行贷款总额达人民币1亿元，部分贷款已逾期未还。 供应商欠款：因现金流紧张，公司拖欠供应商货款达人民币300万元。 员工工资及社保：由于资金链断裂，公司拖欠员工工资及社保费用共计人民币200万元。', metadata={'source': 'example/fake.docx'}), Document(page_content='银行贷款：公司向多家银行贷款总额达人民币1亿元，部分贷款已逾期未还。 供应商欠款：因现金流紧张，公司拖欠供应商货款达人民币300万元。 员工工资及社保：由于资金链断裂，公司拖欠员工工资及社保费用共计人民币200万元。', metadata={'source': 'example/fake.docx'}), Document(page_content='四、债权债务情况\\n\\n宏图科技发展有限公司目前面临的债务问题严峻，具体情况如下：', metadata={'source': 'example/fake.docx'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader, Docx2txtLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#引入上下文压缩相关包\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import  LLMChainExtractor\n",
    "\n",
    "# 定义 ChatDoc 类\n",
    "class ChatDoc():\n",
    "    \n",
    "    def __init__(self):\n",
    "         # 定义类变量\n",
    "        self.doc = None\n",
    "        self.splitText = [] # 用户存储分割后的文本\n",
    "\n",
    "    def getFile(self):\n",
    "        doc = self.doc\n",
    "        loaders = {\n",
    "            \"docx\":Docx2txtLoader,\n",
    "            \"pdf\":PyPDFLoader,\n",
    "            \"xlsx\":UnstructuredExcelLoader,\n",
    "        }\n",
    "        file_extension = doc.split(\".\")[-1]\n",
    "        loader_class = loaders.get(file_extension)\n",
    "        if loader_class:\n",
    "            try:\n",
    "                loader = loader_class(doc)\n",
    "                text = loader.load()\n",
    "                return text\n",
    "            except Exception as e: \n",
    "                print(f\"Error loading {file_extension} files:{e}\") \n",
    "        else:\n",
    "             print(f\"Unsupported file extension: {file_extension}\")\n",
    "             return  None \n",
    "\n",
    "    def splitSentences(self):\n",
    "        \"\"\"\n",
    "        处理文档分割的函数\n",
    "        \"\"\"\n",
    "        full_text = self.getFile() # 获取文档内容\n",
    "        if full_text != None:\n",
    "            # 对文档进行分割\n",
    "            text_split = CharacterTextSplitter(\n",
    "                chunk_size=150,\n",
    "                chunk_overlap=20,\n",
    "            )\n",
    "            texts = text_split.split_documents(full_text)\n",
    "            self.splitText = texts\n",
    "    \n",
    "    def embeddingAndVectorDB(self):\n",
    "        \"\"\"\n",
    "        将分割后的文本块进行向量化并存储到 Chroma 向量数据库中\n",
    "        \"\"\"\n",
    "        # 创建嵌入模型\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        # 创建一个 Chroma 向量数据库实例\n",
    "        db = Chroma.from_documents(\n",
    "            documents = self.splitText, # 分割后文档的列表\n",
    "            embedding = embeddings, # 嵌入模型实例\n",
    "        )\n",
    "        # 返回创建的 Chroma 向量数据库实例\n",
    "        return db\n",
    "    \n",
    "    def askAndFindFiles(self, question):\n",
    "        \"\"\"\n",
    "        LLMChainExtractor 是一种压缩器，用于减少文档的长度，只保留与查询相关的信息。\n",
    "        \"\"\"\n",
    "        db = self.embeddingAndVectorDB()\n",
    "        retriever = db.as_retriever()\n",
    "        # 创建一个 OpenAI 实例，用于文档压缩\n",
    "        llm = OpenAI(temperature=0)\n",
    "        # 创建一个 LLMChainExtractor 实例，它使用 LLM 从检索到的文档中提取相关信息。\n",
    "        compressor = LLMChainExtractor.from_llm(\n",
    "            llm=llm,\n",
    "        )\n",
    "        # 创建上下文压缩检索器\n",
    "        compressor_retriever = ContextualCompressionRetriever(\n",
    "            base_retriever=retriever,\n",
    "            base_compressor=compressor,\n",
    "        )\n",
    "        # 执行检索并返回结果\n",
    "        return compressor_retriever.get_relevant_documents(query=question)\n",
    "\n",
    "\n",
    "# 创建 ChatDoc 实例进行测试\n",
    "chat_doc = ChatDoc()\n",
    "chat_doc.doc = \"./sources/fake.docx\"\n",
    "chat_doc.splitSentences()\n",
    "\n",
    "# 设置下logging查看生成查询\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.DEBUG)\n",
    "unique_doc = chat_doc.askAndFindFiles(\"这间公司的负债有多少？\")\n",
    "print(unique_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.优化3：在向量存储里使用最大边际相似性（MMR）和相似性打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ai-yyds.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='一、公司基本信息\\n\\n名称：宏图科技发展有限公司\\n\\n注册地址：江苏省南京市雨花台区软件大道101号\\n\\n成立日期：2011年5月16日\\n\\n法定代表人：李强\\n\\n注册资本：人民币5000万元\\n\\n员工人数：约200人\\n\\n联系电话：025-88888888\\n\\n电子邮箱：info@hongtutech.cn', metadata={'source': 'example/fake.docx'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader, Docx2txtLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# 定义 ChatDoc 类\n",
    "class ChatDoc():\n",
    "    \n",
    "    def __init__(self):\n",
    "         # 定义类变量\n",
    "        self.doc = None\n",
    "        self.splitText = [] # 用户存储分割后的文本\n",
    "\n",
    "    def getFile(self):\n",
    "        doc = self.doc\n",
    "        loaders = {\n",
    "            \"docx\":Docx2txtLoader,\n",
    "            \"pdf\":PyPDFLoader,\n",
    "            \"xlsx\":UnstructuredExcelLoader,\n",
    "        }\n",
    "        file_extension = doc.split(\".\")[-1]\n",
    "        loader_class = loaders.get(file_extension)\n",
    "        if loader_class:\n",
    "            try:\n",
    "                loader = loader_class(doc)\n",
    "                text = loader.load()\n",
    "                return text\n",
    "            except Exception as e: \n",
    "                print(f\"Error loading {file_extension} files:{e}\") \n",
    "        else:\n",
    "             print(f\"Unsupported file extension: {file_extension}\")\n",
    "             return  None \n",
    "\n",
    "    def splitSentences(self):\n",
    "        \"\"\"\n",
    "        处理文档分割的函数\n",
    "        \"\"\"\n",
    "        full_text = self.getFile() # 获取文档内容\n",
    "        if full_text != None:\n",
    "            # 对文档进行分割\n",
    "            text_split = CharacterTextSplitter(\n",
    "                chunk_size=150,\n",
    "                chunk_overlap=20,\n",
    "            )\n",
    "            texts = text_split.split_documents(full_text)\n",
    "            self.splitText = texts\n",
    "    \n",
    "    def embeddingAndVectorDB(self):\n",
    "        \"\"\"\n",
    "        将分割后的文本块进行向量化并存储到 Chroma 向量数据库中\n",
    "        \"\"\"\n",
    "        # 创建嵌入模型\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        # 创建一个 Chroma 向量数据库实例\n",
    "        db = Chroma.from_documents(\n",
    "            documents = self.splitText, # 分割后文档的列表\n",
    "            embedding = embeddings, # 嵌入模型实例\n",
    "        )\n",
    "        # 返回创建的 Chroma 向量数据库实例\n",
    "        return db\n",
    "    \n",
    "    #提问并找到相关的文本块\n",
    "    def askAndFindFiles(self, question):\n",
    "        db = self.embeddingAndVectorDB()\n",
    "        # 使用 MMR（最大边际相关性）检索类型，提高检索结果的多样性\n",
    "        # retriever = db.as_retriever(search_type=\"mmr\")\n",
    "        # 使用了 similarity_score_threshold （基于相似度得分阈值的检索）作为检索类型，并设置了特定的阈值\n",
    "        # \"score_threshold\": 0.1：指定相似度得分阈值为 0.1。只有相似度得分大于或等于 0.1 的文档才会被返回。\n",
    "        # \"k\": 1：指定返回的文档数量为 1\n",
    "        retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\":.1,\"k\":1})\n",
    "        return retriever.get_relevant_documents(query=question)\n",
    "        \n",
    "\n",
    "# 创建 ChatDoc 实例进行测试\n",
    "chat_doc = ChatDoc()\n",
    "chat_doc.doc = \"./sources/fake.docx\"\n",
    "chat_doc.splitSentences()\n",
    "\n",
    "# 设置下logging查看生成查询\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.DEBUG)\n",
    "unique_doc = chat_doc.askAndFindFiles(\"这家公司的地址在哪里?\")\n",
    "print(unique_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.用自然语言与文档聊天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader, Docx2txtLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# 导入聊天所需要的模块\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 定义 ChatDoc 类\n",
    "class ChatDoc():\n",
    "    \n",
    "    def __init__(self):\n",
    "         # 定义类变量\n",
    "        self.doc = None\n",
    "        self.splitText = [] # 用户存储分割后的文本\n",
    "        self.template =[\n",
    "            (\"system\", \"你是一个处理文档的秘书，你从不说自己是一个大模型或AI助手。你会根据下面提供的上下文内容来继续回来打问题。\\n上下文内容\\b{context}\\n\"),\n",
    "            (\"human\", \"你好！\"),\n",
    "            (\"ai\", \"你好！\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "        self.prompt = ChatPromptTemplate.from_messages(self.template)\n",
    "\n",
    "    def getFile(self):\n",
    "        doc = self.doc\n",
    "        loaders = {\n",
    "            \"docx\":Docx2txtLoader,\n",
    "            \"pdf\":PyPDFLoader,\n",
    "            \"xlsx\":UnstructuredExcelLoader,\n",
    "        }\n",
    "        file_extension = doc.split(\".\")[-1]\n",
    "        loader_class = loaders.get(file_extension)\n",
    "        if loader_class:\n",
    "            try:\n",
    "                loader = loader_class(doc)\n",
    "                text = loader.load()\n",
    "                return text\n",
    "            except Exception as e: \n",
    "                print(f\"Error loading {file_extension} files:{e}\") \n",
    "        else:\n",
    "             print(f\"Unsupported file extension: {file_extension}\")\n",
    "             return  None \n",
    "\n",
    "    def splitSentences(self):\n",
    "        \"\"\"\n",
    "        处理文档分割的函数\n",
    "        \"\"\"\n",
    "        full_text = self.getFile() # 获取文档内容\n",
    "        if full_text != None:\n",
    "            # 对文档进行分割\n",
    "            text_split = CharacterTextSplitter(\n",
    "                chunk_size=150,\n",
    "                chunk_overlap=20,\n",
    "            )\n",
    "            texts = text_split.split_documents(full_text)\n",
    "            self.splitText = texts\n",
    "    \n",
    "    def embeddingAndVectorDB(self):\n",
    "        \"\"\"\n",
    "        将分割后的文本块进行向量化并存储到 Chroma 向量数据库中\n",
    "        \"\"\"\n",
    "        # 创建嵌入模型\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        # 创建一个 Chroma 向量数据库实例\n",
    "        db = Chroma.from_documents(\n",
    "            documents = self.splitText, # 分割后文档的列表\n",
    "            embedding = embeddings, # 嵌入模型实例\n",
    "        )\n",
    "        # 返回创建的 Chroma 向量数据库实例\n",
    "        return db\n",
    "    \n",
    "    #提问并找到相关的文本块\n",
    "    def askAndFindFiles(self, question):\n",
    "        db = self.embeddingAndVectorDB()\n",
    "        # 使用 MMR（最大边际相关性）检索类型，提高检索结果的多样性\n",
    "        # retriever = db.as_retriever(search_type=\"mmr\")\n",
    "        # 使用了 similarity_score_threshold （基于相似度得分阈值的检索）作为检索类型，并设置了特定的阈值\n",
    "        # \"score_threshold\": 0.1：指定相似度得分阈值为 0.1。只有相似度得分大于或等于 0.1 的文档才会被返回。\n",
    "        # \"k\": 1：指定返回的文档数量为 1\n",
    "        retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\":.1,\"k\":1})\n",
    "        return retriever.get_relevant_documents(query=question)\n",
    "        \n",
    "    def chatWithDoc(self, question):\n",
    "        _content = \"\"\n",
    "        context = self.askAndFindFiles(question)\n",
    "        for i in context:\n",
    "            _content += i.page_content\n",
    "        \n",
    "        messages = self.prompt.format_messages(context=_content, question=question)\n",
    "        chat = ChatOpenAI(\n",
    "            model=\"gpt-4\",\n",
    "            temperature=0\n",
    "        )\n",
    "        return chat.invoke(messages)\n",
    "\n",
    "\n",
    "# 创建 ChatDoc 实例进行测试\n",
    "chat_doc = ChatDoc()\n",
    "chat_doc.doc = \"./sources/fake.docx\"\n",
    "chat_doc.splitSentences()\n",
    "\n",
    "# chat_doc.chatWithDoc(\"你是谁？\")\n",
    "# chat_doc.chatWithDoc(\"这家公司目前是盈利还是亏损？\")\n",
    "# chat_doc.chatWithDoc(\"这家公司的主要业务是什么？\")\n",
    "# chat_doc.chatWithDoc(\"公司在什么地方？\")\n",
    "chat_doc.chatWithDoc(\"公司的注册地址在哪？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-chain-wYHJBg1W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
